<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Justin Naldzin - guides</title><link href="http://justinnaldzin.github.io/" rel="alternate"></link><link href="http://justinnaldzin.github.io/feeds/guides.atom.xml" rel="self"></link><id>http://justinnaldzin.github.io/</id><updated>2018-04-07T00:00:00-04:00</updated><entry><title>Replication from AWS RDS MySQL to Google Cloud SQL (without downtime)</title><link href="http://justinnaldzin.github.io/replication-from-aws-rds-mysql-to-google-cloud-sql-without-downtime.html" rel="alternate"></link><published>2018-04-07T00:00:00-04:00</published><updated>2018-04-07T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-04-07:/replication-from-aws-rds-mysql-to-google-cloud-sql-without-downtime.html</id><summary type="html">&lt;p&gt;The following guide performs cloud-to-cloud replication from &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon Relational Database Service (RDS)&lt;/a&gt; to &lt;a href="https://cloud.google.com/sql/docs/"&gt;Google Cloud SQL&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The following guide performs cloud-to-cloud replication from &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon Relational Database Service (RDS)&lt;/a&gt; to &lt;a href="https://cloud.google.com/sql/docs/"&gt;Google Cloud SQL&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/aws_rds.png" alt="RDS" hspace="50"&gt;
&lt;img src="images/logos/gcp_cloudsql.png" alt="Cloud SQL" hspace="50"&gt;
&lt;/p&gt;

&lt;h4&gt;Requirements&lt;/h4&gt;
&lt;p&gt;Replicating AWS RDS instances to Google Cloud SQL requires the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The AWS RDS master must have an Elastic (public) IP address which enables the instance to be reached from the Internet.  Both &lt;a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario4"&gt;VPC&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario6"&gt;non-VPC&lt;/a&gt; networks are supported.&lt;/li&gt;
&lt;li&gt;The AWS RDS database engine must be MySQL v5.5 or v5.6.  PostgreSQL is &lt;a href="https://cloud.google.com/sql/docs/postgres/replication/configure-external-master"&gt;not yet supported&lt;/a&gt; in Cloud SQL.&lt;/li&gt;
&lt;li&gt;Binary logging format must be &lt;code&gt;ROW&lt;/code&gt; on the RDS master requiring an instance restart*.&lt;/li&gt;
&lt;li&gt;*Creating a separate read-replica avoids downtime to the master instance.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;server-id&lt;/code&gt; option must be set to a value of 2 or larger.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few &lt;a href="https://cloud.google.com/sql/docs/mysql/replication/tips#external-master"&gt;additional requirements&lt;/a&gt; for Cloud SQL but the above addresses specifics to default configurations of AWS RDS.&lt;/p&gt;
&lt;h4&gt;Replication types&lt;/h4&gt;
&lt;p&gt;Google Cloud SQL supports &lt;a href="https://cloud.google.com/sql/docs/mysql/replication/"&gt;three replication types&lt;/a&gt; to replicate a master instance to one or more read-replicas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read-Replica (Cloud SQL instances that replicate from a Cloud SQL master instance)&lt;/li&gt;
&lt;li&gt;External Read-Replica (external MySQL instances that are replicating from a Cloud SQL master)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External master (External MySQL instance to Cloud SQL)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;External master&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/sql/docs/mysql/replication/tips#external-master"&gt;External masters&lt;/a&gt; are MySQL instances that are external to Cloud SQL (such as AWS RDS) and serve as masters to a Cloud SQL instance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://cloud.google.com/sql/images/external-master.svg"&gt;&lt;/p&gt;
&lt;h2&gt;AWS RDS MySQL Configuration&lt;/h2&gt;
&lt;h3&gt;AWS Credentials&lt;/h3&gt;
&lt;p&gt;Ensure AWS CLI credentials exist:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;~/.aws/credentials&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;default&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;aws_access_key_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;AKIAIOSFODNN7EXAMPLE
&lt;span class="nv"&gt;aws_secret_access_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

&lt;span class="o"&gt;[&lt;/span&gt;my-named-profile&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;aws_access_key_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;AKIAI44QH8DHBEXAMPLE
&lt;span class="nv"&gt;aws_secret_access_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Use the correct named profile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;AWS_PROFILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-named-profile
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Identify VPC&lt;/h3&gt;
&lt;p&gt;If the RDS instance exists within a VPC, we need to specify that VPC:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# default VPC&lt;/span&gt;
&lt;span class="nv"&gt;VPC_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws ec2 describe-vpcs &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--filters &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;isDefault,Values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;Vpcs[0].VpcId&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# by name&lt;/span&gt;
&lt;span class="nv"&gt;VPC_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws ec2 describe-vpcs &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--filters &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;tag:Name,Values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;My Named VPC&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;Vpcs[0].VpcId&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# manually specify ID&lt;/span&gt;
&lt;span class="nv"&gt;VPC_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;vpc-1a2b3c4d
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create security groups&lt;/h3&gt;
&lt;p&gt;This allows TCP ingress on MySQL port 3306 from specific IPs using CIDR notation.&lt;/p&gt;
&lt;h5&gt;Allow access from specific IPs&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rds-mysql
&lt;span class="nv"&gt;IPS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.35.19.31/32 10.88.52.130/32 10.26.47.1/32 10.24.172.83/32&amp;quot;&lt;/span&gt;
aws ec2 create-security-group &lt;span class="se"&gt;\&lt;/span&gt;
--group-name &lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--description &lt;span class="s2"&gt;&amp;quot;Allow ingress on 3306 for MySQL&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--vpc-id&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$VPC_ID&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; IP in &lt;span class="nv"&gt;$IPS&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
   aws ec2 authorize-security-group-ingress &lt;span class="se"&gt;\&lt;/span&gt;
   --group-name &lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
   --protocol tcp &lt;span class="se"&gt;\&lt;/span&gt;
   --cidr &lt;span class="nv"&gt;$IP&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
   --port &lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="nv"&gt;SECURITY_GROUP_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws ec2 describe-security-groups &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--filters &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;group-name,Values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;SecurityGroups[0].GroupId&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Allow access from local IP&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rds-mysql-local
&lt;span class="nv"&gt;CIDR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;curl -s ifconfig.co&lt;span class="k"&gt;)&lt;/span&gt;/32
aws ec2 create-security-group &lt;span class="se"&gt;\&lt;/span&gt;
--group-name &lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--description &lt;span class="s2"&gt;&amp;quot;Allow ingress on 3306 for MySQL from local IP&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--vpc-id&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$VPC_ID&lt;/span&gt;
aws ec2 authorize-security-group-ingress &lt;span class="se"&gt;\&lt;/span&gt;
--group-name &lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--protocol tcp &lt;span class="se"&gt;\&lt;/span&gt;
--cidr &lt;span class="nv"&gt;$CIDR&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--port &lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="nv"&gt;SECURITY_GROUP_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws ec2 describe-security-groups &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--filters &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;group-name,Values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;SecurityGroups[0].GroupId&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Allow access from anywhere*&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;*Not recommended in production.  Use only for testing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rds-mysql-anywhere
&lt;span class="nv"&gt;CIDR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.0.0.0/0
aws ec2 create-security-group &lt;span class="se"&gt;\&lt;/span&gt;
--group-name &lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--description &lt;span class="s2"&gt;&amp;quot;Allow ingress on 3306 for MySQL from anywhere&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--vpc-id&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$VPC_ID&lt;/span&gt;
aws ec2 authorize-security-group-ingress &lt;span class="se"&gt;\&lt;/span&gt;
--group-id &lt;span class="nv"&gt;$SECURITY_GROUP_ID&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--protocol tcp &lt;span class="se"&gt;\&lt;/span&gt;
--cidr &lt;span class="nv"&gt;$CIDR&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--port &lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="nv"&gt;SECURITY_GROUP_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws ec2 describe-security-groups &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--filters &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;group-name,Values&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$SECURITY_GROUP_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;SecurityGroups[0].GroupId&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create custom parameter group&lt;/h3&gt;
&lt;p&gt;It is a requirement for Cloud SQL to have binary logging enabled on the master as well as setting the config &lt;code&gt;binlog_format=ROW&lt;/code&gt;.  Enabling binary logging &lt;a href="https://cloud.google.com/sql/docs/mysql/replication/tips#bin-log-impact"&gt;impacts the master&lt;/a&gt; by requiring an instance restart.  Existing database connections are lost and must be reestablished.  To avoid instance restart, you would create an RDS read-replica using the following parameter group.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DB_PARAMETER_GROUP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;custom-mysql5-6
aws rds create-db-parameter-group &lt;span class="se"&gt;\&lt;/span&gt;
--db-parameter-group-name &lt;span class="nv"&gt;$DB_PARAMETER_GROUP&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--db-parameter-group-family MySQL5.6 &lt;span class="se"&gt;\&lt;/span&gt;
--description &lt;span class="s2"&gt;&amp;quot;Parameter: binlog_format=ROW&amp;quot;&lt;/span&gt;
aws rds modify-db-parameter-group &lt;span class="se"&gt;\&lt;/span&gt;
--db-parameter-group-name &lt;span class="nv"&gt;$DB_PARAMETER_GROUP&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--parameters &lt;span class="s2"&gt;&amp;quot;ParameterName=binlog_format,ParameterValue=ROW,ApplyMethod=immediate&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;RDS MySQL instance : testing&lt;/h2&gt;
&lt;p&gt;Follow these steps to create a test RDS instance as a &lt;em&gt;proof-of-concept&lt;/em&gt;.  If you already have an RDS instance running, skip to &lt;a href="#rds-mysql-instance-:-production"&gt;RDS MySQL instance : production&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Create MySQL DB instance&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DB_INSTANCE_IDENTIFIER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mysql-instance1
&lt;span class="nv"&gt;DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mytestdb
&lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mysql
&lt;span class="nv"&gt;ENGINE_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;.6.36
&lt;span class="nv"&gt;AVAILABILITY_ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east-1d
&lt;span class="nv"&gt;MASTER_USERNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;root
&lt;span class="nv"&gt;MASTER_USER_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;myrootpass
&lt;span class="nv"&gt;AVAILABILITY_ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east-1a
&lt;span class="nv"&gt;DB_SUBNET_GROUP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;default-vpc-1a2b3c4d
aws rds create-db-instance &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-class db.t2.micro &lt;span class="se"&gt;\&lt;/span&gt;
--storage-type gp2 &lt;span class="se"&gt;\&lt;/span&gt;
--allocated-storage &lt;span class="m"&gt;20&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--availability-zone &lt;span class="nv"&gt;$AVAILABILITY_ZONE&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--engine &lt;span class="nv"&gt;$ENGINE&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--db-name &lt;span class="nv"&gt;$DB_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--master-username &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--master-user-password &lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--vpc-security-group-ids&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECURITY_GROUP_ID&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--db-parameter-group-name &lt;span class="nv"&gt;$DB_PARAMETER_GROUP&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--db-subnet-group-name&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$DB_SUBNET_GROUP&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Wait until instance is available&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;aws rds &lt;span class="nb"&gt;wait&lt;/span&gt; db-instance-available &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Insert test data into database&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;RDS_MASTER_HOSTNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws rds describe-db-instances &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--query DBInstances&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;.Endpoint.Address&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# create table&lt;/span&gt;
curl -s &lt;span class="s2"&gt;&amp;quot;https://api.mockaroo.com/api/57b951e0?count=0&amp;amp;key=3666cca0&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mysql -h &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; &lt;span class="nv"&gt;$DB_NAME&lt;/span&gt;

&lt;span class="c1"&gt;# insert data&lt;/span&gt;
curl -s &lt;span class="s2"&gt;&amp;quot;https://api.mockaroo.com/api/1cf77ef0?count=1000&amp;amp;key=3666cca0&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mysql -h &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; &lt;span class="nv"&gt;$DB_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;RDS MySQL instance : production&lt;/h2&gt;
&lt;h3&gt;Export data with mysqldump&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://cloud.google.com/sql/docs/mysql/import-export/creating-sqldump-csv"&gt;Google Cloud SQL requirements&lt;/a&gt; for creating a SQL dump file.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:  Using the &lt;code&gt;mysqldump --master-data&lt;/code&gt; option is necessary to find the precise binlog coordinates where the backup begins.  This requires &lt;code&gt;SUPER&lt;/code&gt; privileges which RDS does NOT allow.  See &lt;a href="https://stackoverflow.com/a/20645291/9370950"&gt;https://stackoverflow.com/a/20645291/9370950&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The workaround is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create an RDS read-replica of the RDS master&lt;/li&gt;
&lt;li&gt;Stop replication on the RDS read-replica&lt;/li&gt;
&lt;li&gt;Note the replica's binlog coordinates&lt;/li&gt;
&lt;li&gt;Create logical backup using &lt;code&gt;mysqldump&lt;/code&gt; on the RDS read-replica&lt;/li&gt;
&lt;li&gt;Inject the binlog coordinates into the backup file&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Create RDS read-replica&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DB_INSTANCE_IDENTIFIER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mysql-instance1
&lt;span class="nv"&gt;REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DB_INSTANCE_IDENTIFIER&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;-replica&amp;quot;&lt;/span&gt;
aws rds create-db-instance-read-replica &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--source-db-instance-identifier &lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Wait until instance is available&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;aws rds &lt;span class="nb"&gt;wait&lt;/span&gt; db-instance-available &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt;
aws rds describe-db-instances &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--query DBInstances&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;.StatusInfos
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Stop replication&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;MASTER_USERNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;root
&lt;span class="nv"&gt;MASTER_USER_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;myrootpass
&lt;span class="nv"&gt;RDS_REPLICA_HOSTNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;aws rds describe-db-instances &lt;span class="se"&gt;\&lt;/span&gt;
--db-instance-identifier &lt;span class="nv"&gt;$REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--output text &lt;span class="se"&gt;\&lt;/span&gt;
--query DBInstances&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;.Endpoint.Address&lt;span class="k"&gt;)&lt;/span&gt;
mysql -h &lt;span class="nv"&gt;$RDS_REPLICA_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;CALL mysql.rds_stop_replication;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Note the master coordinates of the binlog&lt;/h3&gt;
&lt;p&gt;On the read-replica, run &lt;code&gt;SHOW SLAVE STATUS&lt;/code&gt; to view the master coordinates, noting the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Exec_Master_Log_Pos&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Relay_Master_Log_File&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;EXEC_MASTER_LOG_POS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;mysql -h &lt;span class="nv"&gt;$RDS_REPLICA_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;SHOW SLAVE STATUS\G&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Exec_Master_Log_Pos&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $2 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;RELAY_MASTER_LOG_FILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;mysql -h &lt;span class="nv"&gt;$RDS_REPLICA_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;SHOW SLAVE STATUS\G&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Relay_Master_Log_File&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $2 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create backup of the RDS read-replica&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;BACKUP_FILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rds_backup.sql
&lt;span class="nv"&gt;DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;myproddb
mysqldump --databases &lt;span class="nv"&gt;$DB_NAME&lt;/span&gt; -h &lt;span class="nv"&gt;$RDS_REPLICA_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--flush-privileges &lt;span class="se"&gt;\&lt;/span&gt;
--hex-blob &lt;span class="se"&gt;\&lt;/span&gt;
--skip-triggers &lt;span class="se"&gt;\&lt;/span&gt;
--default-character-set&lt;span class="o"&gt;=&lt;/span&gt;utf8 &lt;span class="se"&gt;\&lt;/span&gt;
--order-by-primary &lt;span class="se"&gt;\&lt;/span&gt;
--single-transaction &amp;gt; &lt;span class="nv"&gt;$BACKUP_FILE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Add binlog coordinates to backup file&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\n\n-- Position to start replication or point-in-time recovery from\nCHANGE MASTER TO MASTER_LOG_FILE=&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$RELAY_MASTER_LOG_FILE&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;, MASTER_LOG_POS=&lt;/span&gt;&lt;span class="nv"&gt;$EXEC_MASTER_LOG_POS&lt;/span&gt;&lt;span class="s2"&gt;;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$BACKUP_FILE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Add RDS specific tables to backup file&lt;/h3&gt;
&lt;p&gt;Add the following to the backup file&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-- RDS tables

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_configuration&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;name&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;value&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;description&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;300&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  PRIMARY KEY &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;name&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_global_status_history&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;collection_end&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  &lt;span class="sb"&gt;`&lt;/span&gt;collection_start&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NULL DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_name&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_value&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_delta&lt;span class="sb"&gt;`&lt;/span&gt; int&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  PRIMARY KEY &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;collection_end&lt;span class="sb"&gt;`&lt;/span&gt;,&lt;span class="sb"&gt;`&lt;/span&gt;variable_name&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

 CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_global_status_history_old&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;
  &lt;span class="sb"&gt;`&lt;/span&gt;collection_end&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  &lt;span class="sb"&gt;`&lt;/span&gt;collection_start&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NULL DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_name&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_value&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;variable_delta&lt;span class="sb"&gt;`&lt;/span&gt; int&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  PRIMARY KEY &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;collection_end&lt;span class="sb"&gt;`&lt;/span&gt;,&lt;span class="sb"&gt;`&lt;/span&gt;variable_name&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

 CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_heartbeat2&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;
  &lt;span class="sb"&gt;`&lt;/span&gt;id&lt;span class="sb"&gt;`&lt;/span&gt; int&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;11&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;value&lt;span class="sb"&gt;`&lt;/span&gt; bigint&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  PRIMARY KEY &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;id&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_history&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;action_timestamp&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  &lt;span class="sb"&gt;`&lt;/span&gt;called_by_user&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;action&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;mysql_version&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_host&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_port&lt;span class="sb"&gt;`&lt;/span&gt; int&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;11&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_user&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;16&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_log_file&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_log_pos&lt;span class="sb"&gt;`&lt;/span&gt; mediumtext,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_ssl&lt;span class="sb"&gt;`&lt;/span&gt; tinyint&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_replication_status&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;action_timestamp&lt;span class="sb"&gt;`&lt;/span&gt; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  &lt;span class="sb"&gt;`&lt;/span&gt;called_by_user&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;action&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;mysql_version&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; NOT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_host&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;master_port&lt;span class="sb"&gt;`&lt;/span&gt; int&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;11&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;rds_sysinfo&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;name&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;25&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL,
  &lt;span class="sb"&gt;`&lt;/span&gt;value&lt;span class="sb"&gt;`&lt;/span&gt; varchar&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; DEFAULT NULL
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;InnoDB DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;latin1&lt;span class="p"&gt;;&lt;/span&gt;

CREATE TABLE &lt;span class="sb"&gt;`&lt;/span&gt;host&lt;span class="sb"&gt;`&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;  
  &lt;span class="sb"&gt;`&lt;/span&gt;Host&lt;span class="sb"&gt;`&lt;/span&gt; char&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; COLLATE utf8_bin NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Db&lt;span class="sb"&gt;`&lt;/span&gt; char&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; COLLATE utf8_bin NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Select_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Insert_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Update_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Delete_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Create_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Drop_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Grant_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;References_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Index_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Alter_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Create_tmp_table_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Lock_tables_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Create_view_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Show_view_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Create_routine_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Alter_routine_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Execute_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  &lt;span class="sb"&gt;`&lt;/span&gt;Trigger_priv&lt;span class="sb"&gt;`&lt;/span&gt; enum&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,&lt;span class="s1"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; CHARACTER SET utf8 NOT NULL DEFAULT &lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;,
  PRIMARY KEY &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;Host&lt;span class="sb"&gt;`&lt;/span&gt;,&lt;span class="sb"&gt;`&lt;/span&gt;Db&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;MyISAM DEFAULT &lt;span class="nv"&gt;CHARSET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;utf8 &lt;span class="nv"&gt;COLLATE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;utf8_bin &lt;span class="nv"&gt;COMMENT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Host privileges;  Merged with database privileges&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create replication user on RDS master&lt;/h3&gt;
&lt;p&gt;Normally in production, you would limit the replication user to be from the read-replica (slave) machine. For example, &lt;code&gt;'$REPLICATION_USER'@'my-read-replica.gcp.googlehost.com'&lt;/code&gt;.  Since the read-replica hasn't been created and the host address is unknown at this point, temporarily allow the replication user from anywhere.  Alternatively you could &lt;a href="https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address"&gt;reserve a static external IP address&lt;/a&gt; within the Google Cloud console and then attach it to the read-replica instance after creation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;REPLICATION_USER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;repl&amp;#39;&lt;/span&gt;
&lt;span class="nv"&gt;REPLICATION_USER_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slavepass&amp;#39;&lt;/span&gt;
mysql -h &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;CREATE USER &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39; IDENTIFIED BY &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER_PASSWORD&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;; GRANT REPLICATION SLAVE ON *.* TO &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;@&amp;#39;%&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Configure Google Cloud SQL External Master&lt;/h2&gt;
&lt;p&gt;This follows &lt;a href="https://cloud.google.com/sql/docs/mysql/replication/configure-external-master"&gt;Google's guidelines&lt;/a&gt; on how to configure an external master.&lt;/p&gt;
&lt;h3&gt;Google Cloud credentials&lt;/h3&gt;
&lt;h5&gt;Create Google Cloud service account&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;gcp-project
&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;gcp-project-id
&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;default
gcloud iam service-accounts create &lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt;
gcloud projects add-iam-policy-binding &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --member &lt;span class="s2"&gt;&amp;quot;serviceAccount:&lt;/span&gt;&lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="s2"&gt;@&lt;/span&gt;&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;&lt;span class="s2"&gt;.iam.gserviceaccount.com&amp;quot;&lt;/span&gt;  --role &lt;span class="s2"&gt;&amp;quot;roles/owner&amp;quot;&lt;/span&gt;
gcloud iam service-accounts keys create ~/.google-cloud-credentials.json --iam-account &lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt;@&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;.iam.gserviceaccount.com
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Set environment variable&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;~/.google-cloud-credentials.json
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;(Optional) Add to bash profile&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\n# Google Cloud\nexport GOOGLE_APPLICATION_CREDENTIALS=~/.google-cloud-credentials.json\n&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create Cloud Storage bucket&lt;/h3&gt;
&lt;p&gt;This bucket will be used to store the &lt;code&gt;mysqldump&lt;/code&gt; backup file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;STORAGE_CLASS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;regional
&lt;span class="nv"&gt;BUCKET_LOCATION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east1
&lt;span class="nv"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-gcp-bucket
gsutil mb -p &lt;span class="nv"&gt;$PROJECT_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-c &lt;span class="nv"&gt;$STORAGE_CLASS&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-l &lt;span class="nv"&gt;$BUCKET_LOCATION&lt;/span&gt; gs://&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;/
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Upload mysqldump file&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;BUCKET_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;cloudsql/mysqldump/&lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt;
gsutil cp &lt;span class="nv"&gt;$BACKUP_FILE&lt;/span&gt; gs://&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;/&lt;span class="nv"&gt;$BUCKET_PATH&lt;/span&gt;/
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create internal master instance&lt;/h3&gt;
&lt;p&gt;The internal master instance (Cloud SQL virtual IP address) and the external master instance (AWS RDS IP address) together make up the master instance for the Cloud SQL replica.  See &lt;a href="https://cloud.google.com/sql/docs/mysql/replication/tips#external-master"&gt;external master configuration&lt;/a&gt; for more details.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;INTERNAL_MASTER_INSTANCE_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$DB_INSTANCE_IDENTIFIER&lt;/span&gt;
&lt;span class="nv"&gt;RDS_MASTER_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;resolveip -s &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;REGION_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east1
&lt;span class="nv"&gt;EXTERNAL_MASTER_DATABASE_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;MYSQL_5_6
&lt;span class="nv"&gt;PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="nv"&gt;ACCESS_TOKEN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;gcloud auth application-default print-access-token&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# Internal master&lt;/span&gt;
curl --header &lt;span class="s2"&gt;&amp;quot;Authorization: Bearer &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ACCESS_TOKEN&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --header &lt;span class="s1"&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--data &lt;span class="s1"&gt;&amp;#39;{&amp;quot;name&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$INTERNAL_MASTER_INSTANCE_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$REGION_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;databaseVersion&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$EXTERNAL_MASTER_DATABASE_VERSION&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;onPremisesConfiguration&amp;quot;: {&amp;quot;hostPort&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$RDS_MASTER_IP&lt;/span&gt;&lt;span class="s2"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;$PORT&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;}}&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-X POST https://www.googleapis.com/sql/v1beta4/projects/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/instances
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create replica instance&lt;/h3&gt;
&lt;p&gt;Cloud SQL First generation instance&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;REPLICA_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_INSTANCE_IDENTIFIER&lt;/span&gt;
&lt;span class="nv"&gt;TIER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;D4

&lt;span class="c1"&gt;# Replica&lt;/span&gt;
curl --header &lt;span class="s2"&gt;&amp;quot;Authorization: Bearer &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ACCESS_TOKEN&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --header &lt;span class="s1"&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--data &lt;span class="s1"&gt;&amp;#39;{&amp;quot;replicaConfiguration&amp;quot;: {&amp;quot;mysqlReplicaConfiguration&amp;quot;: {&amp;quot;username&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;password&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER_PASSWORD&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;dumpFilePath&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gs://&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET_PATH&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;$BACKUP_FILE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;}}, &amp;quot;settings&amp;quot;: {&amp;quot;tier&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$TIER&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;,&amp;quot;activationPolicy&amp;quot;: &amp;quot;ALWAYS&amp;quot;}, &amp;quot;databaseVersion&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$EXTERNAL_MASTER_DATABASE_VERSION&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;masterInstanceName&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$INTERNAL_MASTER_INSTANCE_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$REGION_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;}&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
-X POST https://www.googleapis.com/sql/v1beta4/projects/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/instances
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Wait until instance is available&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Wait for CREATE operation&lt;/span&gt;
&lt;span class="nv"&gt;OPERATION_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;gcloud sql operations list --instance&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt; --filter&lt;span class="o"&gt;=&lt;/span&gt;operationType:&lt;span class="s2"&gt;&amp;quot;CREATE&amp;quot;&lt;/span&gt; --format&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;value(name)&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
gcloud sql operations &lt;span class="nb"&gt;wait&lt;/span&gt; &lt;span class="nv"&gt;$OPERATION_ID&lt;/span&gt;

&lt;span class="c1"&gt;# Wait for all operations&lt;/span&gt;
gcloud sql operations &lt;span class="nb"&gt;wait&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;gcloud sql operations list --instance&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt; --uri&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Patch replica&lt;/h3&gt;
&lt;p&gt;Assign IPv4 address, change to synchronous replication, and add CIDR ingress network (IP address of RDS master), then restart&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud sql instances patch &lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--assign-ip &lt;span class="se"&gt;\&lt;/span&gt;
--replication&lt;span class="o"&gt;=&lt;/span&gt;SYNCHRONOUS &lt;span class="se"&gt;\&lt;/span&gt;
--authorized-networks&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RDS_MASTER_IP&lt;/span&gt;/32
gcloud sql instances restart &lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should now limit the replication user on the RDS master to be from the read-replica (slave) machine. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;GCP_REPLICA_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;gcloud sql instances describe &lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt; --format&lt;span class="o"&gt;=&lt;/span&gt;json &lt;span class="p"&gt;|&lt;/span&gt; jq -r &lt;span class="s1"&gt;&amp;#39;.ipAddresses[0].ipAddress&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
mysql -h &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;RENAME USER &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;@&amp;#39;%&amp;#39; TO &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICATION_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;@&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$GCP_REPLICA_IP&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Check replication status&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud sql instances describe &lt;span class="nv"&gt;$REPLICA_NAME&lt;/span&gt; 
gcloud sql instances describe &lt;span class="nv"&gt;$INTERNAL_MASTER_INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Accessing the read-replica&lt;/h3&gt;
&lt;p&gt;Cloud SQL prevents creating users on read-replicas.  Creating a user on the external master propagates down to the replica, allowing access to the read-replica.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;REPLICA_USER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;myreplica
&lt;span class="nv"&gt;REPLICA_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;myreplicapass
mysql -h &lt;span class="nv"&gt;$RDS_MASTER_HOSTNAME&lt;/span&gt; -u &lt;span class="nv"&gt;$MASTER_USERNAME&lt;/span&gt; -p&lt;span class="nv"&gt;$MASTER_USER_PASSWORD&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;CREATE USER &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39; IDENTIFIED BY &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_PASSWORD&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;; GRANT ALL PRIVILEGES ON &lt;/span&gt;&lt;span class="nv"&gt;$DB_NAME&lt;/span&gt;&lt;span class="s2"&gt;.* TO &amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;$REPLICA_USER&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;@&amp;#39;%&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure SSL&lt;/h3&gt;
&lt;p&gt;Configure SSL for the connections from Cloud SQL to external masters.  See:  &lt;a href="https://cloud.google.com/sql/docs/mysql/configure-ssl-instance"&gt;https://cloud.google.com/sql/docs/mysql/configure-ssl-instance&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Next steps for full migration to GCP&lt;/h2&gt;
&lt;p&gt;The following describe the steps to take to fully migrate off AWS and onto GCP&lt;/p&gt;
&lt;h4&gt;1. Stop AWS RDS read/write services&lt;/h4&gt;
&lt;h4&gt;2. Promote the Cloud SQL replica&lt;/h4&gt;
&lt;p&gt;Promoting a replica to a stand-alone Cloud SQL instance is an irreversible action. Once promoted, an instance cannot be converted back to a read-replica.&lt;/p&gt;
&lt;h4&gt;3. (Optional) Migrate and Upgrade Google Cloud SQL&lt;/h4&gt;
&lt;p&gt;Migrating to a Google Cloud SQL Second Generation instance offers higher performance and storage capacity at a lower cost.  Additionally upgrade the version of MySQL to 5.6 or 5.7 all in the same migration step.&lt;/p&gt;
&lt;h4&gt;4. Point read/write applications to the GCP Cloud SQL instance and start services&lt;/h4&gt;</content><category term="aws"></category><category term="google cloud"></category><category term="rds"></category><category term="cloud sql"></category><category term="mysql"></category></entry><entry><title>Jupyter Notebook with PySpark on a Google Cloud Dataproc cluster</title><link href="http://justinnaldzin.github.io/jupyter-notebook-with-pyspark-on-a-google-cloud-dataproc-cluster.html" rel="alternate"></link><published>2018-04-05T00:00:00-04:00</published><updated>2018-04-05T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-04-05:/jupyter-notebook-with-pyspark-on-a-google-cloud-dataproc-cluster.html</id><summary type="html">&lt;p&gt;This guide shows how to use an initialization action to install Jupyter notebook and the PySpark kernel on a Cloud Dataproc cluster.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;This guide shows how to use an initialization action to install Jupyter notebook and the PySpark kernel on a Cloud Dataproc cluster.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/jupyter.png" alt="Jupyter" hspace="20" vspace="20" valign="middle"&gt;
&lt;img src="images/logos/card_gcp_clouddataproc.png" alt="Cloud Dataproc" hspace="20" vspace="20" valign="middle"&gt;
&lt;img src="images/logos/spark.png" alt="Apache Spark" hspace="20" vspace="20" valign="middle"&gt;
&lt;/p&gt;

&lt;h2&gt;Create a Cloud Storage bucket&lt;/h2&gt;
&lt;p&gt;The Dataproc staging bucket will be used to store Jupyter notebooks at &lt;code&gt;gs://$BUCKET_NAME/notebooks/&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-project-name
&lt;span class="nv"&gt;STORAGE_CLASS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;regional
&lt;span class="nv"&gt;BUCKET_LOCATION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east1
&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;dataproc-spark-cluster-1
&lt;span class="nv"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BUCKET_LOCATION&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
gsutil mb -p &lt;span class="nv"&gt;$PROJECT_NAME&lt;/span&gt; -c &lt;span class="nv"&gt;$STORAGE_CLASS&lt;/span&gt; -l &lt;span class="nv"&gt;$BUCKET_LOCATION&lt;/span&gt; gs://&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;/
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create Dataproc cluster with Jupyter initialization action&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-project-id
&lt;span class="nv"&gt;REGION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET_LOCATION&lt;/span&gt;
&lt;span class="nv"&gt;ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REGION&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-d
gcloud dataproc clusters create &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; --project &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --bucket &lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt; --region &lt;span class="nv"&gt;$REGION&lt;/span&gt; --zone &lt;span class="nv"&gt;$ZONE&lt;/span&gt; --initialization-actions gs://dataproc-initialization-actions/jupyter/jupyter.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Dynamic Port Forwarding&lt;/h2&gt;
&lt;p&gt;To avoid opening a publicly reachable port on the cluster's master node, use dynamic port forwarding (via an SSH tunnel using the SOCKS protocol) to connect your browser to the Jupyter notebook running on your cluster's master node.&lt;/p&gt;
&lt;h2&gt;Setup SSH tunnel&lt;/h2&gt;
&lt;p&gt;Create an SSH tunnel to your cluster's master node from port 10000 on your localhost machine&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;MASTER_HOST_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-m
&lt;span class="nv"&gt;PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10000&lt;/span&gt;
gcloud compute ssh &lt;span class="nv"&gt;$MASTER_HOST_NAME&lt;/span&gt; --project &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --zone&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$ZONE&lt;/span&gt; -- -D &lt;span class="nv"&gt;$PORT&lt;/span&gt; -N
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Open Jupyter notebook in your browser&lt;/h2&gt;
&lt;p&gt;Launch a new browser that connects through the SSH tunnel (using the SOCKS protocol) to the Jupyter notebook application running on your cluster's master node.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/Applications/Google&lt;span class="se"&gt;\ &lt;/span&gt;Chrome.app/Contents/MacOS/Google&lt;span class="se"&gt;\ &lt;/span&gt;Chrome &lt;span class="s2"&gt;&amp;quot;http://&lt;/span&gt;&lt;span class="nv"&gt;$MASTER_HOST_NAME&lt;/span&gt;&lt;span class="s2"&gt;:8123&amp;quot;&lt;/span&gt; --proxy-server&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;socks5://localhost:&lt;/span&gt;&lt;span class="nv"&gt;$PORT&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --host-resolver-rules&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MAP * 0.0.0.0 , EXCLUDE localhost&amp;quot;&lt;/span&gt; --user-data-dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creating a new notebook shows the PySpark kernel is now available&lt;/p&gt;
&lt;h2&gt;Dataproc Web Interfaces&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Yarn Resource Manager:  http://master-host-name:8088&lt;/li&gt;
&lt;li&gt;HDFS NameNode:  http://master-host-name:9870&lt;/li&gt;
&lt;li&gt;Spark Master UI:  http://master-host-name:4040&lt;/li&gt;
&lt;li&gt;Spark History Server:  http://master-host-name:18080&lt;/li&gt;
&lt;li&gt;Hadoop Job History Server:  http://master-host-name:19888&lt;/li&gt;
&lt;/ul&gt;</content><category term="google cloud"></category><category term="dataproc"></category><category term="jupyter"></category><category term="pyspark"></category><category term="apache spark"></category></entry><entry><title>Jupyter Notebook with Apache Spark</title><link href="http://justinnaldzin.github.io/jupyter-notebook-with-apache-spark.html" rel="alternate"></link><published>2018-04-01T00:00:00-04:00</published><updated>2018-04-01T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-04-01:/jupyter-notebook-with-apache-spark.html</id><summary type="html">&lt;p&gt;This guide installs Python, Jupyter, Java, Scala, and Apache Spark&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This guide installs the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.6.4&lt;/li&gt;
&lt;li&gt;Jupyter&lt;/li&gt;
&lt;li&gt;Java 1.8.0_162&lt;/li&gt;
&lt;li&gt;Scala 2.12.5&lt;/li&gt;
&lt;li&gt;Apache Spark 2.3.0&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/jupyter.png" alt="Jupyter" valign="bottom" hspace="30" vspace="10"&gt;
&lt;img src="images/logos/spark.png" alt="Spark" valign="top" hspace="20" vspace="10"&gt;
&lt;/p&gt;

&lt;h2&gt;Setup&lt;/h2&gt;
&lt;h4&gt;Install Python 3.6&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install python3
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Jupyter&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip3 install jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Java&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew update
brew tap caskroom/cask
brew cask install java
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Apache Spark and Scala&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew update
brew install scala
brew install apache-spark
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Update &lt;code&gt;.bash_profile&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Add the following to &lt;code&gt;~/.bash_profile&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# For Apache Spark&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; which java &amp;gt; /dev/null&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;/usr/libexec/java_home&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="c1"&gt;# For ipython notebook and pyspark integration&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; which pyspark &amp;gt; /dev/null&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
  &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SPARK_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/local/Cellar/apache-spark/2.3.0/libexec/&amp;quot;&lt;/span&gt;
  &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt;/python:&lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt;/python/build:&lt;span class="nv"&gt;$PYTHONPATH&lt;/span&gt;
  &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt;/python/lib/py4j-0.10.6-src.zip:&lt;span class="nv"&gt;$PYTHONPATH&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Launch Jupyter Notebook&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter notebook
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install &lt;code&gt;findspark&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Within a new Notebook using the Python 3 kernel, use &lt;a href="https://github.com/minrk/findspark"&gt;findspark&lt;/a&gt; to add PySpark to &lt;code&gt;sys.path&lt;/code&gt; at runtime&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;!&lt;/span&gt;&lt;span class="n"&gt;pip3&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;findspark&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;findspark&lt;/span&gt;
&lt;span class="n"&gt;findspark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Example script&lt;/h2&gt;
&lt;p&gt;The following example script loads tables from an external MySQL database&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Download the &lt;a href="https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.zip"&gt;JDBC driver for MySQL (Connector/J)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;PYSPARK_SUBMIT_ARGS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--packages mysql:mysql-connector-java:5.1.46 pyspark-shell&amp;quot;&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkConf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkSession&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;

&lt;span class="c1"&gt;# Create SparkContext and SQLContext&lt;/span&gt;
&lt;span class="n"&gt;appName&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;PySpark app&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkConf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setAppName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sqlContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SQLContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config&lt;/span&gt;
&lt;span class="n"&gt;host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;10.0.0.1&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;database&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mydatabase&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mytable&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;jdbc:mysql://{}/{}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;database&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;properties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;password&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;driver&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;com.mysql.jdbc.Driver&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sqlContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jdbc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;properties&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;properties&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="jupyter"></category><category term="spark"></category><category term="python"></category><category term="scala"></category><category term="java"></category></entry><entry><title>Google Cloud Datalab on a Cloud Dataproc cluster</title><link href="http://justinnaldzin.github.io/google-cloud-datalab-on-a-cloud-dataproc-cluster.html" rel="alternate"></link><published>2018-03-30T00:00:00-04:00</published><updated>2018-03-30T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-30:/google-cloud-datalab-on-a-cloud-dataproc-cluster.html</id><summary type="html">&lt;p&gt;This guide shows how to use an initialization action to install Cloud Datalab on a Cloud Dataproc cluster.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;This guide shows how to use an initialization action to install Cloud Datalab on a Cloud Dataproc cluster.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/card_gcp_clouddatalab.png" alt="Cloud Datalab" hspace="20" vspace="20" valign="middle"&gt;
&lt;img src="images/logos/card_gcp_clouddataproc.png" alt="Cloud Dataproc" hspace="20" vspace="20" valign="middle"&gt;
&lt;img src="images/logos/spark.png" alt="Apache Spark" hspace="20" vspace="20" valign="middle"&gt;
&lt;/p&gt;

&lt;h2&gt;Create a Cloud Storage bucket&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: The Dataproc staging bucket is NOT used to store Datalab notebooks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-project-name
&lt;span class="nv"&gt;STORAGE_CLASS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;regional
&lt;span class="nv"&gt;BUCKET_LOCATION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;us-east1
&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;dataproc-spark-cluster-1
&lt;span class="nv"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BUCKET_LOCATION&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
gsutil mb -p &lt;span class="nv"&gt;$PROJECT_NAME&lt;/span&gt; -c &lt;span class="nv"&gt;$STORAGE_CLASS&lt;/span&gt; -l &lt;span class="nv"&gt;$BUCKET_LOCATION&lt;/span&gt; gs://&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;/
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create Dataproc cluster with Datalab initialization action&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-project-id
&lt;span class="nv"&gt;REGION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET_LOCATION&lt;/span&gt;
&lt;span class="nv"&gt;ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REGION&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-d
gcloud dataproc clusters create &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; --project &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --bucket &lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt; --region &lt;span class="nv"&gt;$REGION&lt;/span&gt; --zone &lt;span class="nv"&gt;$ZONE&lt;/span&gt; --initialization-actions gs://dataproc-initialization-actions/datalab/datalab.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Dynamic Port Forwarding&lt;/h2&gt;
&lt;p&gt;To avoid opening a publicly reachable port on the cluster's master node, use dynamic port forwarding (via an SSH tunnel using the SOCKS protocol) to connect your browser to the Datalab notebook running on your cluster's master node.&lt;/p&gt;
&lt;h2&gt;Setup SSH tunnel&lt;/h2&gt;
&lt;p&gt;Create an SSH tunnel to your cluster's master node from port 10000 on your localhost machine&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;MASTER_HOST_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;-m
&lt;span class="nv"&gt;PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10000&lt;/span&gt;
gcloud compute ssh &lt;span class="nv"&gt;$MASTER_HOST_NAME&lt;/span&gt; --project &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --zone&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$ZONE&lt;/span&gt; -- -D &lt;span class="nv"&gt;$PORT&lt;/span&gt; -N
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Open Datalab in your browser&lt;/h2&gt;
&lt;p&gt;Launch a new browser that connects through the SSH tunnel (using the SOCKS protocol) to the Datalab application running on your cluster's master node.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/Applications/Google&lt;span class="se"&gt;\ &lt;/span&gt;Chrome.app/Contents/MacOS/Google&lt;span class="se"&gt;\ &lt;/span&gt;Chrome &lt;span class="s2"&gt;&amp;quot;http://&lt;/span&gt;&lt;span class="nv"&gt;$MASTER_HOST_NAME&lt;/span&gt;&lt;span class="s2"&gt;:8080&amp;quot;&lt;/span&gt; --proxy-server&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;socks5://localhost:&lt;/span&gt;&lt;span class="nv"&gt;$PORT&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; --host-resolver-rules&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MAP * 0.0.0.0 , EXCLUDE localhost&amp;quot;&lt;/span&gt; --user-data-dir&lt;span class="o"&gt;=&lt;/span&gt;/tmp/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creating a new notebook with the Python3 kernel has the &lt;code&gt;SparkContext&lt;/code&gt; available as &lt;code&gt;sc&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Dataproc Web Interfaces&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Yarn Resource Manager:  http://master-host-name:8088&lt;/li&gt;
&lt;li&gt;HDFS NameNode:  http://master-host-name:9870&lt;/li&gt;
&lt;li&gt;Spark Master UI:  http://master-host-name:4040&lt;/li&gt;
&lt;li&gt;Spark History Server:  http://master-host-name:18080&lt;/li&gt;
&lt;li&gt;Hadoop Job History Server:  http://master-host-name:19888&lt;/li&gt;
&lt;/ul&gt;</content><category term="google cloud"></category><category term="dataproc"></category><category term="datalab"></category><category term="pyspark"></category><category term="apache spark"></category></entry><entry><title>Setup Google Cloud Datalab</title><link href="http://justinnaldzin.github.io/setup-google-cloud-datalab.html" rel="alternate"></link><published>2018-03-25T00:00:00-04:00</published><updated>2018-03-25T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-25:/setup-google-cloud-datalab.html</id><summary type="html">&lt;p&gt;The following guide shows how to use the &lt;code&gt;datalab&lt;/code&gt; command line tool to set up and open Google Cloud Datalab.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;The following guide shows how to use the &lt;code&gt;datalab&lt;/code&gt; command line tool to set up and open Google Cloud Datalab.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/card_gcp_clouddatalab.png" alt="Cloud Datalab"&gt;
&lt;/p&gt;

&lt;h4&gt;Update to the latest gcloud version&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud components update
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Datalab&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud components install datalab
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Create Datalab instance&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;datalab create&lt;/code&gt; command also creates the following Google Cloud Platform resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;datalab-network&lt;/code&gt; network&lt;/li&gt;
&lt;li&gt;A firewall rule on the &lt;code&gt;datalab-network&lt;/code&gt; allowing incoming SSH connections&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;datalab-notebooks&lt;/code&gt; Google Cloud Source Repository&lt;/li&gt;
&lt;li&gt;The persistent disk for storing Cloud Datalab notebooks&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;INSTANCE_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-datalab-instance
datalab create &lt;span class="nv"&gt;$INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Stop Datalab instance to avoid incurring unnecessary costs&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;datalab stop &lt;span class="nv"&gt;$INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Connect to Datalab after the instance is stopped&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;datalab connect &lt;span class="nv"&gt;$INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete Datalab VM instance and its Persistent Disk&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;datalab delete --delete-disk &lt;span class="nv"&gt;$INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete Datalab VM instance without deleting the Persistent Disk&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;datalab delete --keep-disk &lt;span class="nv"&gt;$INSTANCE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;</content><category term="google cloud"></category><category term="datalab"></category></entry><entry><title>Creating Google Cloud Dataproc Clusters</title><link href="http://justinnaldzin.github.io/creating-google-cloud-dataproc-clusters.html" rel="alternate"></link><published>2018-03-20T00:00:00-04:00</published><updated>2018-03-20T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-20:/creating-google-cloud-dataproc-clusters.html</id><summary type="html">&lt;p&gt;&lt;a href="https://cloud.google.com/dataproc/"&gt;Cloud Dataproc&lt;/a&gt; is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Cloud Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://cloud.google.com/dataproc/"&gt;Cloud Dataproc&lt;/a&gt; is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Cloud Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/gcp_clouddataproc.png" alt="Cloud Dataproc"&gt;
&lt;/p&gt;

&lt;h2&gt;Create a Cloud Dataproc cluster&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-cluster
gcloud dataproc clusters create &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above command creates a cluster with default Cloud Dataproc service settings.  Noteable cluster settings that are worth changing are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Staging bucket&lt;/li&gt;
&lt;li&gt;Staging buckets are used for miscellaneous configuration and control files as well as output from the Cloud SDK gcloud dataproc clusters &lt;a href="https://cloud.google.com/dataproc/docs/support/diagnose-command"&gt;diagnose&lt;/a&gt; command. &lt;/li&gt;
&lt;li&gt;Network&lt;/li&gt;
&lt;li&gt;Define a Compute Engine network to configure Firewall rules for connecting to the cluster and viewing the UI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Describe cluster&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud dataproc clusters describe &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Delete cluster&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Deleting the cluster does NOT delete the associated Cloud Storage bucket.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud dataproc clusters delete &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Delete cluster and storage bucket&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;BUCKET_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;gcloud dataproc clusters describe &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; --format&lt;span class="o"&gt;=&lt;/span&gt;json &lt;span class="p"&gt;|&lt;/span&gt; jq -r &lt;span class="s1"&gt;&amp;#39;.config.configBucket&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
gcloud dataproc clusters delete &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; 
gsutil -m rm -r gs://&lt;span class="nv"&gt;$BUCKET_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="google cloud"></category><category term="dataproc"></category></entry><entry><title>Pushing Docker images to Google Container Registry</title><link href="http://justinnaldzin.github.io/pushing-docker-images-to-google-container-registry.html" rel="alternate"></link><published>2018-03-17T00:00:00-04:00</published><updated>2018-03-17T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-17:/pushing-docker-images-to-google-container-registry.html</id><summary type="html">&lt;p&gt;The following guide explains how to build Docker images and push to &lt;a href="https://cloud.google.com/container-registry/docs/"&gt;Google Container Registry&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;The following guide explains how to build Docker images and push to &lt;a href="https://cloud.google.com/container-registry/docs/"&gt;Google Container Registry&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/card_gcp_containerregistry.png" alt="Container Registry"&gt;
&lt;/p&gt;

&lt;h3&gt;Build Docker image&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;IMAGE_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-image
docker build -t &lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt; .
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Tag the image with a registry name&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-gcp-project
&lt;span class="nv"&gt;TAG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.0
docker tag &lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt; gcr.io/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt;:&lt;span class="nv"&gt;$TAG&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Push the image to Container Registry&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud docker -- push gcr.io/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt;:&lt;span class="nv"&gt;$TAG&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;View the image's registry name in web browser&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open http://gcr.io/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Delete the image from Container Registry&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud container images delete gcr.io/&lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_NAME&lt;/span&gt;:&lt;span class="nv"&gt;$TAG&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="google cloud"></category><category term="container registry"></category><category term="docker"></category></entry><entry><title>Deploying Debezium on Google Kubernetes Engine</title><link href="http://justinnaldzin.github.io/deploying-debezium-on-google-kubernetes-engine.html" rel="alternate"></link><published>2018-03-10T00:00:00-05:00</published><updated>2018-03-10T00:00:00-05:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-10:/deploying-debezium-on-google-kubernetes-engine.html</id><summary type="html">&lt;p&gt;&lt;a href="https://cloud.google.com/kubernetes-engine/"&gt;Google Kubernetes Engine&lt;/a&gt; provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The environment Kubernetes Engine provides consists of multiple machines (specifically, Google Compute Engine instances) grouped together to form a container cluster.  &lt;a href="http://debezium.io"&gt;Debezium&lt;/a&gt; is an open source distributed platform for &lt;a href="https://en.wikipedia.org/wiki/Change_data_capture"&gt;change data capture&lt;/a&gt;. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/kubernetes-engine/"&gt;Google Kubernetes Engine&lt;/a&gt; provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The environment Kubernetes Engine provides consists of multiple machines (specifically, Google Compute Engine instances) grouped together to form a container cluster.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/gcp_containerengine.png" alt="Google Cloud Platform" hspace="50"&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href="http://debezium.io"&gt;Debezium&lt;/a&gt; is an open source distributed platform for &lt;a href="https://en.wikipedia.org/wiki/Change_data_capture"&gt;change data capture&lt;/a&gt;. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.&lt;/p&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;The following assumes you have already followed the instructions for &lt;a href="configuring-gcloud"&gt;configuring gcloud&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Activate gcloud configuration&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud config configurations list
gcloud config configurations activate default
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Create a Kubernetes cluster&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-project_id
&lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;test-cluster1
gcloud container clusters create &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--machine-type &lt;span class="s2"&gt;&amp;quot;n1-standard-2&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--cluster-version &lt;span class="s2"&gt;&amp;quot;1.8.8-gke.0&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--project &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creating a cluster on Google Kubernetes Engine generates a kubeconfig entry and automatically switches to that context.&lt;/p&gt;
&lt;h4&gt;Show kubectl contexts&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl config get-contexts
kubectl config use-context &amp;lt;gke_project_region_cluster&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Manual kubectl configuration&lt;/h4&gt;
&lt;p&gt;If you created a cluster using GCP Console or using gcloud on a different machine, you need to make the cluster's credentials available to &lt;code&gt;kubectl&lt;/code&gt; in your current environment.&lt;/p&gt;
&lt;p&gt;To pass the cluster's credentials to &lt;code&gt;kubectl&lt;/code&gt;, run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud container clusters get-credentials &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This also automatically switches to that context.&lt;/p&gt;
&lt;h4&gt;Show kubectl contexts&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl config get-contexts
kubectl config use-context &amp;lt;gke_project_region_cluster&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;View cluster info&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl cluster-info
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Clone Debezium Kubernetes repo&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/debezium/debezium-kubernetes.git
&lt;span class="nb"&gt;cd&lt;/span&gt; debezium-kubernetes
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:  The &lt;code&gt;debezium-kubernetes&lt;/code&gt; repo uses outdated docker images and an outdated fabric8 version.  Need to update each &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Change the docker images from &lt;code&gt;0.1-SNAPSHOT&lt;/code&gt; to &lt;code&gt;0.8-SNAPSHOT&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.debezium&lt;span class="nt"&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;version&amp;gt;&lt;/span&gt;0.8-SNAPSHOT&lt;span class="nt"&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Change the fabric 8 version from &lt;code&gt;2.2.115&lt;/code&gt; to &lt;code&gt;2.2.215&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;fabric8.version&amp;gt;&lt;/span&gt;2.2.215&lt;span class="nt"&gt;&amp;lt;/fabric8.version&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Build with Maven&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mvn clean install
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Deploy with Maven&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mvn fabric8:apply
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Get pod details&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl get pods
kubectl describe pods
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Kafka&lt;/h2&gt;
&lt;h4&gt;Create a &lt;code&gt;schema-changes&lt;/code&gt; topic for Debezium's MySQL connector&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;ticketmonster
&lt;span class="nv"&gt;TOPIC&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;schema-changes.&lt;span class="nv"&gt;$DB_NAME&lt;/span&gt;
&lt;span class="nv"&gt;KAFKA_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^kafka-0 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /usr/bin/kafka-topics  --create --zookeeper zk-0.zk-svc.default.svc.cluster.local:2181 --replication-factor &lt;span class="m"&gt;3&lt;/span&gt; --partitions &lt;span class="m"&gt;1&lt;/span&gt; --topic &lt;span class="nv"&gt;$TOPIC&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;MySQL&lt;/h2&gt;
&lt;h4&gt;Connect to the MySQL command line&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;MYSQL_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep Running &lt;span class="p"&gt;|&lt;/span&gt; grep ^mysql-0 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;MYSQL_POD_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl describe pod &lt;span class="nv"&gt;$MYSQL_POD_NAME&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep IP &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $2 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; -it &lt;span class="nv"&gt;$MYSQL_POD_NAME&lt;/span&gt; -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h&lt;span class="nv"&gt;$MYSQL_POD_IP&lt;/span&gt; -P3306 -uroot -padmin
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Execute SQL&lt;/h4&gt;
&lt;p&gt;Skip this (see note below)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#kubectl exec -it $MYSQL_POD_NAME -- bash -c &amp;quot;curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE: The &lt;code&gt;GRANT&lt;/code&gt; statements from the above script breaks replication.  Also this script references a different &lt;code&gt;inventory&lt;/code&gt; database than the &lt;code&gt;ticketmonster&lt;/code&gt; database that the &lt;code&gt;mysql56&lt;/code&gt; image includes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Instead use these statements that work&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;GRANT&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RELOAD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;SHOW&lt;/span&gt; &lt;span class="n"&gt;DATABASES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;REPLICATION&lt;/span&gt; &lt;span class="n"&gt;SLAVE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;REPLICATION&lt;/span&gt; &lt;span class="n"&gt;CLIENT&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;TO&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replicator&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;IDENTIFIED&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replpass&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;GRANT&lt;/span&gt; &lt;span class="k"&gt;ALL&lt;/span&gt; &lt;span class="k"&gt;PRIVILEGES&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;ticketmonster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;TO&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;ticket&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;ticketmonster&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Proceed with the rest of the SQL script located &lt;a href="https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Start Kafka Connect and Debezium&lt;/h2&gt;
&lt;h4&gt;Expose API&lt;/h4&gt;
&lt;h6&gt;In a new shell&lt;/h6&gt;
&lt;p&gt;Expose the API for the Kafka Connect cluster via pod port-forwarding (forward the pod's 8083 port to our local machine)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONNECT_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^connect-0 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl port-forward &lt;span class="nv"&gt;$CONNECT_POD_NAME&lt;/span&gt; &lt;span class="m"&gt;8083&lt;/span&gt;:8083
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;View Kafka Connect logs&lt;/h4&gt;
&lt;h6&gt;In a new shell&lt;/h6&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONNECT_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^connect-0 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl logs -f &lt;span class="nv"&gt;$CONNECT_POD_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Create a Debezium connector using the Kafka Connect service's REST API&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -i -X POST -H &lt;span class="s2"&gt;&amp;quot;Accept:application/json&amp;quot;&lt;/span&gt; -H &lt;span class="s2"&gt;&amp;quot;Content-Type:application/json&amp;quot;&lt;/span&gt; http://localhost:8083/connectors/ -d &lt;span class="s1"&gt;&amp;#39;{ &amp;quot;name&amp;quot;: &amp;quot;ticketmonster-connector&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;connector.class&amp;quot;: &amp;quot;io.debezium.connector.mysql.MySqlConnector&amp;quot;, &amp;quot;tasks.max&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;database.hostname&amp;quot;: &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$MYSQL_POD_IP&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;quot;, &amp;quot;database.port&amp;quot;: &amp;quot;3306&amp;quot;, &amp;quot;database.user&amp;quot;: &amp;quot;replicator&amp;quot;, &amp;quot;database.password&amp;quot;: &amp;quot;replpass&amp;quot;, &amp;quot;database.server.id&amp;quot;: &amp;quot;184054&amp;quot;, &amp;quot;database.server.name&amp;quot;: &amp;quot;mysql-server-1&amp;quot;, &amp;quot;database.binlog&amp;quot;: &amp;quot;mysql-bin.000001&amp;quot;, &amp;quot;database.whitelist&amp;quot;: &amp;quot;ticketmonster&amp;quot;, &amp;quot;database.history.kafka.bootstrap.servers&amp;quot;: &amp;quot;kafka-0.kafka-svc.default.svc.cluster.local:9093&amp;quot;, &amp;quot;database.history.kafka.topic&amp;quot;: &amp;quot;schema-changes.ticketmonster&amp;quot; } }&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Consume all event streams from the Kafka topic&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;KAFKA_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^kafka-0 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /usr/bin/kafka-console-consumer --bootstrap-server kafka-0.kafka-svc.default.svc.cluster.local:9093 --topic mysql-server-1.ticketmonster.customers --from-beginning --property print.key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /usr/bin/kafka-console-consumer --bootstrap-server kafka-1.kafka-svc.default.svc.cluster.local:9093 --topic mysql-server-1.ticketmonster.customers --from-beginning --property print.key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;List Kafka topics&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl &lt;span class="nb"&gt;exec&lt;/span&gt;  &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /usr/bin/kafka-topics --list --zookeeper zk-0.zk-svc.default.svc.cluster.local:2181
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete connector&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -X DELETE http://localhost:8083/connectors/ticketmonster-connector
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Cleanup&lt;/h2&gt;
&lt;h4&gt;Delete all resources&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl delete configmaps,statefulsets,services,deployments,pods,secrets,serviceaccounts,roles,rolebindings,pvc --all
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete the cluster&lt;/h4&gt;
&lt;p&gt;Delete the Kubernetes Engine container cluster using the gcloud command-line tool&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud container clusters delete &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This also deletes the entries in the &lt;code&gt;kubectl&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h4&gt;Manually delete kubectl configuration&lt;/h4&gt;
&lt;p&gt;To delete the &lt;code&gt;kubectl&lt;/code&gt; entries manually, delete the cluster and context and unset the user from &lt;code&gt;kubectl&lt;/code&gt; config&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl config delete-cluster &amp;lt;gke_project_region_cluster&amp;gt;
kubectl config delete-context &amp;lt;gke_project_region_cluster&amp;gt;
kubectl config &lt;span class="nb"&gt;unset&lt;/span&gt; users.&amp;lt;gke_project_region_cluster&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Check kubectl configuration&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat ~/.kube/config
&lt;/pre&gt;&lt;/div&gt;</content><category term="debezium"></category><category term="kubernetes"></category><category term="minikube"></category><category term="change data capture"></category><category term="kafka"></category></entry><entry><title>Deploying Debezium on Kubernetes locally via Minikube</title><link href="http://justinnaldzin.github.io/deploying-debezium-on-kubernetes-locally-via-minikube.html" rel="alternate"></link><published>2018-03-05T00:00:00-05:00</published><updated>2018-03-05T00:00:00-05:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-05:/deploying-debezium-on-kubernetes-locally-via-minikube.html</id><summary type="html">&lt;p&gt;&lt;a href="http://debezium.io"&gt;Debezium&lt;/a&gt; is an open source distributed platform for &lt;a href="https://en.wikipedia.org/wiki/Change_data_capture"&gt;change data capture&lt;/a&gt;. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://debezium.io"&gt;Debezium&lt;/a&gt; is an open source distributed platform for &lt;a href="https://en.wikipedia.org/wiki/Change_data_capture"&gt;change data capture&lt;/a&gt;. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.&lt;/p&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;The following assumes you have &lt;a href="running-kubernetes-locally-via-minikube"&gt;Kubernetes running locally via Minikube&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Start minikube cluster&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;minikube start
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Clone Debezium Kubernetes repo&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/debezium/debezium-kubernetes.git
&lt;span class="nb"&gt;cd&lt;/span&gt; debezium-kubernetes
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:  The &lt;code&gt;debezium-kubernetes&lt;/code&gt; repo uses outdated docker images and an outdated fabric8 version.  Need to update each &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Change the docker images from &lt;code&gt;0.1-SNAPSHOT&lt;/code&gt; to &lt;code&gt;0.8-SNAPSHOT&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;io.debezium&lt;span class="nt"&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;version&amp;gt;&lt;/span&gt;0.8-SNAPSHOT&lt;span class="nt"&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Change the fabric 8 version from &lt;code&gt;2.2.115&lt;/code&gt; to &lt;code&gt;2.2.215&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;fabric8.version&amp;gt;&lt;/span&gt;2.2.215&lt;span class="nt"&gt;&amp;lt;/fabric8.version&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Build with Maven&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mvn clean install
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Deploy with Maven&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mvn fabric8:apply
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Get pod details&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl get pods
kubectl describe pods
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Kafka&lt;/h2&gt;
&lt;h4&gt;Create a &lt;code&gt;schema-changes&lt;/code&gt; topic for Debezium's MySQL connector&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;ticketmonster
&lt;span class="nv"&gt;TOPIC&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;schema-changes.&lt;span class="nv"&gt;$DB_NAME&lt;/span&gt;
&lt;span class="nv"&gt;KAFKA_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep kafka &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor &lt;span class="m"&gt;1&lt;/span&gt; --partitions &lt;span class="m"&gt;1&lt;/span&gt; --topic &lt;span class="nv"&gt;$TOPIC&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;MySQL&lt;/h2&gt;
&lt;h4&gt;Connect to the MySQL command line&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;MYSQL_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep Running &lt;span class="p"&gt;|&lt;/span&gt; grep ^mysql &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;MYSQL_POD_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl describe pod &lt;span class="nv"&gt;$MYSQL_POD_NAME&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep IP &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $2 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; -it &lt;span class="nv"&gt;$MYSQL_POD_NAME&lt;/span&gt; -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h&lt;span class="nv"&gt;$MYSQL_POD_IP&lt;/span&gt; -P3306 -uroot -padmin
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Execute SQL&lt;/h4&gt;
&lt;p&gt;Skip this (see note below)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#kubectl exec -it $MYSQL_POD_NAME -- bash -c &amp;quot;curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE: The &lt;code&gt;GRANT&lt;/code&gt; statements from the above script breaks replication.  Also this script references a different &lt;code&gt;inventory&lt;/code&gt; database than the &lt;code&gt;ticketmonster&lt;/code&gt; database that the &lt;code&gt;mysql56&lt;/code&gt; image includes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Instead use these statements that work&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;GRANT&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RELOAD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;SHOW&lt;/span&gt; &lt;span class="n"&gt;DATABASES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;REPLICATION&lt;/span&gt; &lt;span class="n"&gt;SLAVE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;REPLICATION&lt;/span&gt; &lt;span class="n"&gt;CLIENT&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;TO&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replicator&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;IDENTIFIED&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replpass&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;GRANT&lt;/span&gt; &lt;span class="k"&gt;ALL&lt;/span&gt; &lt;span class="k"&gt;PRIVILEGES&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;ticketmonster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;TO&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;ticket&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;USE&lt;/span&gt; &lt;span class="n"&gt;ticketmonster&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Proceed with the rest of the SQL script located &lt;a href="https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Start Kafka Connect and Debezium&lt;/h2&gt;
&lt;h4&gt;Expose API&lt;/h4&gt;
&lt;h6&gt;In a new shell&lt;/h6&gt;
&lt;p&gt;Expose the API for the Kafka Connect cluster via pod port-forwarding (forward the pod's 8083 port to our local machine)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONNECT_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^connect &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl port-forward &lt;span class="nv"&gt;$CONNECT_POD_NAME&lt;/span&gt; &lt;span class="m"&gt;8083&lt;/span&gt;:8083
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;View Kafka Connect logs&lt;/h4&gt;
&lt;h6&gt;In a new shell&lt;/h6&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONNECT_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^connect &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl logs -f &lt;span class="nv"&gt;$CONNECT_POD_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Create a Debezium connector using the Kafka Connect service's REST API&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -i -X POST -H &lt;span class="s2"&gt;&amp;quot;Accept:application/json&amp;quot;&lt;/span&gt; -H &lt;span class="s2"&gt;&amp;quot;Content-Type:application/json&amp;quot;&lt;/span&gt; http://localhost:8083/connectors/ -d &lt;span class="s1"&gt;&amp;#39;{ &amp;quot;name&amp;quot;: &amp;quot;ticketmonster-connector&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;connector.class&amp;quot;: &amp;quot;io.debezium.connector.mysql.MySqlConnector&amp;quot;, &amp;quot;tasks.max&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;database.hostname&amp;quot;: &amp;quot;mysql&amp;quot;, &amp;quot;database.port&amp;quot;: &amp;quot;3306&amp;quot;, &amp;quot;database.user&amp;quot;: &amp;quot;replicator&amp;quot;, &amp;quot;database.password&amp;quot;: &amp;quot;replpass&amp;quot;, &amp;quot;database.server.id&amp;quot;: &amp;quot;184054&amp;quot;, &amp;quot;database.server.name&amp;quot;: &amp;quot;mysql-server-1&amp;quot;, &amp;quot;database.binlog&amp;quot;: &amp;quot;mysql-bin.000001&amp;quot;, &amp;quot;database.whitelist&amp;quot;: &amp;quot;ticketmonster&amp;quot;, &amp;quot;database.history.kafka.bootstrap.servers&amp;quot;: &amp;quot;kafka:9092&amp;quot;, &amp;quot;database.history.kafka.topic&amp;quot;: &amp;quot;schema-changes.ticketmonster&amp;quot; } }&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Consume all event streams from the Kafka topic&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;KAFKA_POD_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;kubectl get pod &lt;span class="p"&gt;|&lt;/span&gt; grep -i running &lt;span class="p"&gt;|&lt;/span&gt; grep ^kafka &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{ print $1 }&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic mysql-server-1.ticketmonster.customers --from-beginning --property print.key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;List Kafka topics&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl &lt;span class="nb"&gt;exec&lt;/span&gt; &lt;span class="nv"&gt;$KAFKA_POD_NAME&lt;/span&gt; -- /kafka/bin/kafka-topics.sh --list --zookeeper zookeeper:2181
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete connector&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -X DELETE http://localhost:8083/connectors/ticketmonster-connector
&lt;/pre&gt;&lt;/div&gt;</content><category term="debezium"></category><category term="kubernetes"></category><category term="minikube"></category><category term="change data capture"></category><category term="kafka"></category></entry><entry><title>Running Kubernetes locally via Minikube</title><link href="http://justinnaldzin.github.io/running-kubernetes-locally-via-minikube.html" rel="alternate"></link><published>2018-03-01T00:00:00-05:00</published><updated>2018-03-01T00:00:00-05:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-03-01:/running-kubernetes-locally-via-minikube.html</id><summary type="html">&lt;p&gt;&lt;a href="https://kubernetes.io"&gt;Kubernetes&lt;/a&gt; is an open-source system for automating deployment, scaling, and management of containerized applications.  &lt;a href="https://github.com/kubernetes/minikube"&gt;Minikube&lt;/a&gt; is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io"&gt;Kubernetes&lt;/a&gt; is an open-source system for automating deployment, scaling, and management of containerized applications.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/kubernetes/minikube"&gt;Minikube&lt;/a&gt; is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="images/logos/kubernetes_logo.png" alt="Kubernetes" hspace="100" vspace="10"&gt;
&lt;img src="images/logos/kubernetes_name.png" alt="Kubernetes" hspace="100" vspace="10"&gt;
&lt;/p&gt;

&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;h4&gt;Install Java 8&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Java 9 fails as of this writing&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew tap caskroom/versions
brew update
brew cask install java8
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install kubectl&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install kubectl
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install VirtualBox&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew cask install virtualbox
brew cask install virtualbox-extension-pack
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Minikube&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew cask install minikube
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;kubectl Configuration&lt;/h3&gt;
&lt;p&gt;The kubectl config file is located at:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;~/.kube/config&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/user/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: &lt;span class="o"&gt;{}&lt;/span&gt;
users:
- name: minikube
  user:
    as-user-extra: &lt;span class="o"&gt;{}&lt;/span&gt;
    client-certificate: /Users/user/.minikube/client.crt
    client-key: /Users/user/.minikube/client.key
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Hello World&lt;/h2&gt;
&lt;h4&gt;Start minikube cluster&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;minikube start
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Starting minikube automatically generates a kubeconfig entry and switches to that context.&lt;/p&gt;
&lt;h4&gt;Show kubectl contexts&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl config get-contexts
kubectl config use-context minikube
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;View cluster info&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl cluster-info
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Run an image creating a deployment&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl run hello-minikube --image&lt;span class="o"&gt;=&lt;/span&gt;k8s.gcr.io/echoserver:1.4 --port&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8080&lt;/span&gt;
kubectl expose deployment hello-minikube --type&lt;span class="o"&gt;=&lt;/span&gt;NodePort
kubectl get pod
curl &lt;span class="k"&gt;$(&lt;/span&gt;minikube service hello-minikube --url&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Kubernetes Dashboard&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;minikube dashboard
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Delete deployment&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl delete deployment hello-minikube
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Stopping a Cluster&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;minikube stop
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Deleting a Cluster&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;minikube delete
&lt;/pre&gt;&lt;/div&gt;</content><category term="kubernetes"></category><category term="minikube"></category><category term="kubectl"></category></entry><entry><title>Configuring gcloud</title><link href="http://justinnaldzin.github.io/configuring-gcloud.html" rel="alternate"></link><published>2018-02-01T00:00:00-05:00</published><updated>2018-02-01T00:00:00-05:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2018-02-01:/configuring-gcloud.html</id><summary type="html">&lt;p&gt;Configuring gcloud&lt;/p&gt;</summary><content type="html">&lt;p align="center"&gt;
&lt;img src="images/logos/gcp_googlecloud_vertical.png" alt="Google Cloud Platform" hspace="50"&gt;
&lt;/p&gt;

&lt;h1&gt;Configuring gcloud with user credentials&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Repeat these steps if you need multiple configurations for different projects and accounts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Create a gcloud named configuration&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONFIG_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-config
gcloud config configurations create &lt;span class="nv"&gt;$CONFIG_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Initialize gcloud&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud init
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Follow the prompts and ensure the config is now activated &lt;/p&gt;
&lt;h3&gt;List configurations&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud config configurations list
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h1&gt;Configuring gcloud to use a service account&lt;/h1&gt;
&lt;h3&gt;Creating a service account&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;gcp-project-id
&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;my-service-account
gcloud iam service-accounts create &lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt; --display-name &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Granting roles to service accounts&lt;/h3&gt;
&lt;h5&gt;Add a role to the service account&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud projects add-iam-policy-binding &lt;span class="nv"&gt;$PROJECT_ID&lt;/span&gt; --member&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&amp;#39;&lt;/span&gt; --role&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;roles/owner&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Alternatively add a group to the service account with a role&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;GROUP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;group:my.group@domain.com
gcloud iam service-accounts add-iam-policy-binding &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;@&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.iam.gserviceaccount.com --member&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$GROUP&lt;/span&gt; --role&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;roles/owner&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create and download a private key&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;KEY_FILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;_&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.json
gcloud iam service-accounts keys create &lt;span class="nv"&gt;$KEY_FILE&lt;/span&gt; --iam-account &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;@&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.iam.gserviceaccount.com
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create a gcloud named configuration&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CONFIG_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SERVICE_ACCOUNT_NAME&lt;/span&gt;
gcloud config configurations create &lt;span class="nv"&gt;$CONFIG_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Activate service account&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud auth activate-service-account &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SERVICE_ACCOUNT_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;@&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PROJECT_ID&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.iam.gserviceaccount.com --key-file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY_FILE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ensure the config is now activated &lt;/p&gt;
&lt;h3&gt;List configurations&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud config configurations list
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h1&gt;Generating application default credentials&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: This creates a file located at: &lt;code&gt;/Users/user/.config/gcloud/application_default_credentials.json&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud auth application-default login
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h1&gt;RBAC (Role-Based Access Control) for Google Kubernetes Engine&lt;/h1&gt;
&lt;h3&gt;Setting up RBAC with &lt;code&gt;ClusterRoleBinding&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Create a &lt;code&gt;ClusterRoleBinding&lt;/code&gt; that gives your Google identity a &lt;code&gt;cluster-admin&lt;/code&gt; role to gain full control over every resource in the cluster and in all namespaces.  This is needed before attempting to create additional Role or ClusterRole permissions.  See the &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control"&gt;RBAC documentation&lt;/a&gt; for more info.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user &lt;span class="k"&gt;$(&lt;/span&gt;gcloud config get-value account&lt;span class="k"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="google cloud platform"></category><category term="gcloud"></category></entry><entry><title>Data Centric Information Technology Career Paths</title><link href="http://justinnaldzin.github.io/data-centric-information-technology-career-paths.html" rel="alternate"></link><published>2017-06-02T00:00:00-04:00</published><updated>2017-06-02T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2017-06-02:/data-centric-information-technology-career-paths.html</id><summary type="html">&lt;p&gt;A list of data centric career paths and job descriptions within current IT organizations.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Career Paths&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#big-data-engineer"&gt;Big Data Engineer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database-manager"&gt;Database Manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database-developer"&gt;Database Developer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database-administrator"&gt;Database Administrator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-analyst/report-writer"&gt;Data Analyst/Report Writer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-architect"&gt;Data Architect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-modeler"&gt;Data Modeler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-scientist"&gt;Data Scientist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-warehouse-manager"&gt;Data Warehouse Manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-warehouse-analyst"&gt;Data Warehouse Analyst&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#business-intelligence-analyst"&gt;Business Intelligence Analyst&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#electronic-data-interchange-(edi)-specialist"&gt;Electronic Data Interchange (EDI) Specialist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#portal-administrator"&gt;Portal Administrator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Big Data Engineer&lt;/h2&gt;
&lt;p&gt;Big data engineers communicate with business users and data scientists to understand the business objectives and translate those objectives into data-processing workflows. Big data engineers should have strong knowledge of statistics, extensive programming experience, ideally in Python or Java, and the ability to design and implement solutions for big data challenges. Knowledge and experience in data mining, processing large amounts of raw data, and designing and maintaining relational databases for storage and data acquisition are desired. Experience with NoSQL is preferred. This individual communicates directly with business users and data scientists to understand objectives and create data-processing workflows. Employers often require a bachelors degree in a related field and four to six years of experience.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gathering and processing raw data and translating analyses&lt;/li&gt;
&lt;li&gt;Evaluating new data sources for acquisition and integration&lt;/li&gt;
&lt;li&gt;Designing and implementing relational databases for storage and processing&lt;/li&gt;
&lt;li&gt;Working directly with the engineering team to integrate data processing and business objectives&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Database Manager&lt;/h2&gt;
&lt;p&gt;Database managers must have an in-depth understanding of all aspects of database technology. Employers generally look for applicants with at least a bachelors degree and five years of experience in an Oracle, Microsoft SQL Server, IBM DB2 or similar environment, along with multiyear experience in a technical management position. Database managers need to be creative, analytical thinkers who can not only lead a team of database professionals but also effectively communicate, plan information system strategy and make presentations to senior IT managers.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maintaining and supporting a companys database environment&lt;/li&gt;
&lt;li&gt;Providing input to a chief technology officer or chief information officer regarding company data standards and practices&lt;/li&gt;
&lt;li&gt;Developing and managing departmental budgets&lt;/li&gt;
&lt;li&gt;Making personnel decisions and work assignments&lt;/li&gt;
&lt;li&gt;Managing capacity planning, disaster recovery and performance analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Database Developer&lt;/h2&gt;
&lt;p&gt;Database developers need a thorough understanding of relational database theory and practice. They must be analytical and adept at problem solving. They also should be good communicators. A bachelors degree in computer science or a related field is often sought, although database experience can be substituted with some employers. Familiarity and experience with major enterprise database programs, such as Microsoft SQL Server, Oracle or IBM DB2, are essential, and professional certification (Microsoft Certified Database Administrator or Oracle Database Administrator Certified Professional, for example) in these programs is a plus. Because many web applications now interface with databases, experience in internet technologies is also valuable.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developing database objects and structures for data storage, retrieval and reporting according to project specifications&lt;/li&gt;
&lt;li&gt;Implementing and testing database design and functionality, and tuning for performance&lt;/li&gt;
&lt;li&gt;Providing support to database administrators and interfacing with business users to ensure the database is satisfying business requirements&lt;/li&gt;
&lt;li&gt;Designing and developing back-end database interfaces to web and e-commerce applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Database Administrator&lt;/h2&gt;
&lt;p&gt;Candidates for the database administrator role need a strong technical foundation in database structure, configuration, installation and practice. Employers seek individuals with knowledge and experience in major relational database languages and applications, such as Microsoft SQL Server, Oracle and IBM DB2. At least two years of postsecondary education is typically required. Professional certifications from Microsoft, Oracle and others are also valuable. Effective database administrators must have keen attention to detail, a strong customer service orientation and the ability to work as part of a team.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Managing, monitoring and maintaining company databases&lt;/li&gt;
&lt;li&gt;Making requested changes, updates and modifications to database structure and data&lt;/li&gt;
&lt;li&gt;Ensuring database integrity, stability and system availability&lt;/li&gt;
&lt;li&gt;Maintaining database backup and recovery infrastructure&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Analyst/Report Writer&lt;/h2&gt;
&lt;p&gt;Strong analytical, quantitative and problem-solving abilities are required for this position, along with thorough knowledge of relational database theory and practice. Employers look for a bachelors degree in computer science, information systems or a related field, plus several years of experience working with major database platforms, such as Microsoft SQL Server, Oracle and IBM DB2. In addition, excellent communication skills and the ability to work both independently and collaboratively with data systems teams are required.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analyzing complex data systems and documenting data elements, data flow, relationships and dependencies&lt;/li&gt;
&lt;li&gt;Developing automated and reusable routines for extracting requested information from database systems&lt;/li&gt;
&lt;li&gt;Compiling detailed reports using data reporting tools such as Crystal Reports, and making recommendations based on their findings&lt;/li&gt;
&lt;li&gt;Working in partnership with business analysts, data architects and database developers to build data transactional and warehousing systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Architect&lt;/h2&gt;
&lt;p&gt;Candidates for data architect positions require a high level of analytical and creative skills, along with in-depth knowledge of data systems and database methodology, design and modeling. They must be able to communicate effectively in order to plan and coordinate data resources. Working knowledge of network management, distributed databases and processing, application architecture, and performance management is highly valued. Employers generally seek a bachelors degree in computer science or a related field, as well as experience with Oracle, Microsoft SQL Server or other databases in various operating system environments such as Unix, Linux, Solaris and Microsoft Windows.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understanding and evaluating business requirements and translating them into specific database solutions&lt;/li&gt;
&lt;li&gt;Creating data design models, database architecture and data repository design&lt;/li&gt;
&lt;li&gt;Working with the systems and database administration staff to implement, coordinate and maintain enterprisewide data architecture&lt;/li&gt;
&lt;li&gt;Providing leadership in establishing and documenting data standards&lt;/li&gt;
&lt;li&gt;Creating and testing database prototypes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Modeler&lt;/h2&gt;
&lt;p&gt;Data modelers must possess excellent data analysis and problem-solving skills, and be able to both communicate effectively and work as part of a team. Employers normally request a bachelors degree in computer science, IT or mathematics, in addition to several years of relevant data management experience. Candidates should be familiar with data modeling tools and methodologies and be knowledgeable in database system applications, stored procedures and data warehousing.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analyzing organizational data requirements and creating logical and physical models of data flow&lt;/li&gt;
&lt;li&gt;Interviewing key project stakeholders, documenting findings and making detailed recommendations&lt;/li&gt;
&lt;li&gt;Working with database administrators and reporting teams to ensure the availability of standard and ad hoc data reporting in a production environment&lt;/li&gt;
&lt;li&gt;Addressing data quality issues with clients and management&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Scientist&lt;/h2&gt;
&lt;p&gt;Data scientists must have a range of mathematical and analytical skills, as well as business acumen. Big data scientists analyze and integrate multiple data sets and make recommendations based on their findings. Experience in programming languages  commonly Python or Java  is often required, as is a Ph.D.&lt;/p&gt;
&lt;p&gt;Typical duties include:
- Gathering and processing raw data
- Providing analysis to leaders in order to support business decisions
- Developing metrics and prototypes that can be used to drive business decisions
- Identifying emerging trends and opportunities for business growth&lt;/p&gt;
&lt;h2&gt;Data Warehouse Manager&lt;/h2&gt;
&lt;p&gt;The data warehouse manager role requires an in-depth background in database theory and practice combined with hands-on experience in data warehousing technology. Managers should have excellent analytical abilities, as well as project management experience. Proficiency in warehousing tools and architecture is a must, as is technical proficiency in database languages and applications such as Oracle, Microsoft SQL Server and IBM DB2. A bachelors degree in computer science or the equivalent, along with five or more years of experience in a data warehousing environment and three or more years in technical personnel management, are often prerequisites.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing, developing and maintaining data warehouses and data mart systems&lt;/li&gt;
&lt;li&gt;Working with database developers, administrators and managers to ensure that data systems conform to enterprise data architecture and strategy&lt;/li&gt;
&lt;li&gt;Developing and implementing strategies for gathering data from operational databases and third-party vendors for inclusion in the warehouse&lt;/li&gt;
&lt;li&gt;Providing leadership in managing technical resources and staff&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Warehouse Analyst&lt;/h2&gt;
&lt;p&gt;Data warehouse analysts must have excellent research, analysis and problem-solving skills, as well as good oral and written communication abilities. A bachelors degree in computer science or a related field, along with extensive knowledge of relational database theory and three to five years of work experience in database systems, are typical prerequisites. Employers also seek candidates who possess experience with data modeling and architecture. A professional certification in a database application such as Microsoft SQL Server or Oracle also is valuable.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collecting, analyzing, mining and leveraging data stored in data warehouses&lt;/li&gt;
&lt;li&gt;Researching and recommending technology solutions related to data storage, reporting, importing and other areas&lt;/li&gt;
&lt;li&gt;Working with business analysts to translate data requirements into logical data models&lt;/li&gt;
&lt;li&gt;Defining user interfaces for interacting with data warehouses and data marts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Business Intelligence Analyst&lt;/h2&gt;
&lt;p&gt;Candidates for business intelligence analyst positions need a strong background in all aspects of database technology, with an emphasis on the use of analytical and reporting tools. Employers seek a bachelors degree in computer science, information systems or engineering, as well as several years of experience with database queries, stored procedure writing, Online Analytical Processing (OLAP) and data cube technology. Excellent written and oral communication skills are a must.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing and developing enterprisewide data analysis and reporting solutions&lt;/li&gt;
&lt;li&gt;Reviewing and analyzing data from multiple internal and external sources&lt;/li&gt;
&lt;li&gt;Communicating analysis results and making recommendations to senior management&lt;/li&gt;
&lt;li&gt;Developing data cleansing rules&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Electronic Data Interchange (EDI) Specialist&lt;/h2&gt;
&lt;p&gt;EDI specialists should have a solid background in information systems technology and working knowledge of data communication protocols. They must be detail-oriented, have excellent problem-solving skills and have the ability to work independently. A bachelors degree in computer science or a related discipline is normally required. In addition, employers typically look for several years of IT-related experience, plus three or more years with EDI systems administration, design, analysis and development.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implementing and monitoring EDI systems, including data mapping, translation and interface&lt;/li&gt;
&lt;li&gt;Coordinating relations with and serving as a liaison to internal users, vendors and other external partners with respect to data interchange standards&lt;/li&gt;
&lt;li&gt;Performing system testing and quality control checks&lt;/li&gt;
&lt;li&gt;Developing and maintaining EDI documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Portal Administrator&lt;/h2&gt;
&lt;p&gt;Portal administrators must have the ability to analyze and solve complex problems, as well as extensive knowledge of enterprise web applications, services, systems and supporting technologies. Portal administrators may interact with a wide range of technical and nontechnical colleagues, so candidates should have excellent written and verbal communication skills. Three to five years of systems administration experience may be required. Many portal administrator positions require experience installing and configuring IBM WebSphere Application Server and related products.&lt;/p&gt;
&lt;p&gt;Typical duties include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrating functional requirements into portal application development&lt;/li&gt;
&lt;li&gt;Managing user access to portal resources&lt;/li&gt;
&lt;li&gt;Deploying and managing portlet applications&lt;/li&gt;
&lt;li&gt;Ensuring reliability and availability of enterprise web environments&lt;/li&gt;
&lt;/ul&gt;</content><category term="information technology"></category></entry><entry><title>Apache Spark integration with Jupyter Notebook</title><link href="http://justinnaldzin.github.io/apache-spark-integration-with-jupyter-notebook.html" rel="alternate"></link><published>2017-05-16T00:00:00-04:00</published><updated>2017-05-16T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2017-05-16:/apache-spark-integration-with-jupyter-notebook.html</id><summary type="html">&lt;p&gt;This guide explains multiple ways to install Apache Spark 2.x locally and integrate with Jupyter Notebook by installing various Spark kernels.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;p&gt;This guide explains multiple ways to install Apache Spark 2.x locally and integrate with Jupyter Notebook by installing various Spark kernels.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#apache-spark-2.x-overview"&gt;Apache Spark 2.x overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#jupyter-notebook-overview"&gt;Jupyter Notebook overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#required-packages"&gt;Install the required packages&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python 3.5+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#java"&gt;Java SE Development Kit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-apache-spark"&gt;Install Apache Spark&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#pre-built"&gt;Pre-built&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#source-code"&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#set-environment-variables"&gt;Set environment variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-jupyter-notebook"&gt;Install Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-a-spark-kernel-for-jupyter-notebook"&gt;Install a Spark kernel for Jupyter Notebook&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#pyspark with ipythonkernel"&gt;PySpark with IPythonKernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#apache-toree"&gt;Apache Toree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sparkmagic"&gt;Sparkmagic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Apache Spark 2.x overview&lt;/h2&gt;
&lt;p&gt;Apache Spark is an open-source cluster-computing framework.  Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.  The release of Spark 2.0 included a number of significant improvements including unifying DataFrame and DataSet, replacing SQLContext and HiveContext with the SparkSession entry point, and much more.  As of this writing, Spark's latest release is 2.1.1.&lt;/p&gt;
&lt;h2&gt;Jupyter Notebook overview&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://jupyter.org/"&gt;Jupyter Notebook&lt;/a&gt; is a web-based interactive computational environment in which you can combine code execution, rich text, mathematics, plots and rich media to create a notebook.  The actual Jupyter notebook is nothing more than a JSON document containing an ordered list of input/output cells.  Jupyter notebooks an be converted to a number of open standard output formats including HTML, presentation slides, LaTeX, PDF, ReStructuredText, Markdown, and Python.&lt;/p&gt;
&lt;p&gt;Jupyter Notebook has support for over 40 programming languages, with the most popular being Python, R, Julia and Scala.  The different components of Jupyter include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter Notebook App&lt;/li&gt;
&lt;li&gt;Jupter documents&lt;/li&gt;
&lt;li&gt;kernels&lt;/li&gt;
&lt;li&gt;Notebook Dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Be sure to check out the &lt;a href="http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/index.html"&gt;Jupyter Notebook beginner guide&lt;/a&gt; to learn more, including &lt;a href="https://jupyter.readthedocs.io/en/latest/index.html"&gt;how to install Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Additionally check out some &lt;a href="https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/"&gt;Jupyter Notebook tips, tricks and shortcuts&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Required packages&lt;/h2&gt;
&lt;p&gt;Integrating Spark with Jupyter Notebook requires the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.5+&lt;/li&gt;
&lt;li&gt;Java SE Development Kit&lt;/li&gt;
&lt;li&gt;Apache Spark 2.x&lt;/li&gt;
&lt;li&gt;Jupyter Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Python&lt;/h4&gt;
&lt;p&gt;Download and Install Python 3
 - &lt;a href="https://www.python.org/downloads/"&gt;download link&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Java&lt;/h4&gt;
&lt;p&gt;Java 7+ is required for Spark which you can download from Oracle's website&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.java.com/en/download/faq/java_mac.xml"&gt;macOS download link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html"&gt;Linux download link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Install Apache Spark&lt;/h2&gt;
&lt;p&gt;There are two types of Spark packages available to download:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-built for Apache Hadoop 2.7 and later&lt;/li&gt;
&lt;li&gt;Source code&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pre-built&lt;/h4&gt;
&lt;p&gt;The pre-built package is the simplest option.&lt;/p&gt;
&lt;p&gt;On the &lt;a href="http://spark.apache.org/downloads.html"&gt;Spark downloads page&lt;/a&gt;, choose to download the zipped Spark package pre-built for Apache Hadoop 2.7+.  Unzip the .tgz file and move the folder to your home directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget -qO- https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz &lt;span class="p"&gt;|&lt;/span&gt; tar xvz -C ~/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a symbolic link from this folder to &lt;code&gt;~/spark&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ln -s ~/spark-2.1.1-bin-hadoop2.7 ~/spark
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Source code&lt;/h4&gt;
&lt;p&gt;Building from the source code offers the ability to configure specific components to install and their version numbers.&lt;/p&gt;
&lt;p&gt;On the &lt;a href="http://spark.apache.org/downloads.html"&gt;Spark downloads page&lt;/a&gt;, choose to download the zipped Spark source code package.  Unzip the .tgz file and move the folder to your home directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget -qO- https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1.tgz &lt;span class="p"&gt;|&lt;/span&gt; tar xvz -C ~/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a symbolic link from this folder to &lt;code&gt;~/spark&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ln -s ~/spark-2.1.1 ~/spark
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are two options for building Spark.  The first one uses the Scala Build Tool (sbt) which needs to be installed&lt;/p&gt;
&lt;h5&gt;macOS:&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install sbt
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Linux:&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl https://bintray.com/sbt/rpm/rpm &lt;span class="p"&gt;|&lt;/span&gt; sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
yum -y install sbt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Navigate to the directory where you unzipped Spark and build Spark.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/spark
sbt assembly
sbt package
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The second option for building Spark is with maven&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;MAVEN_OPTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; ~/spark/
./dev/change-scala-version.sh &lt;span class="m"&gt;2&lt;/span&gt;.12  &lt;span class="c1"&gt;# specify Scala v2.12&lt;/span&gt;
./build/mvn -Pyarn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -Dscala-2.12 -Dhadoop.version&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.7.0 -DskipTests clean package
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Set environment variables&lt;/h2&gt;
&lt;h5&gt;macOS:&lt;/h5&gt;
&lt;p&gt;Add the environment variables to your bash profile located at &lt;code&gt;~/.bash_profile&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;echo &amp;#39;export SPARK_HOME=~/spark&amp;#39; &amp;gt;&amp;gt; ~/.bash_profile
echo &amp;#39;export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip&amp;#39; &amp;gt;&amp;gt; ~/.bash_profile
echo &amp;#39;export PATH=$PATH:$SPARK_HOME/bin&amp;#39; &amp;gt;&amp;gt; ~/.bash_profile
echo &amp;#39;export PYSPARK_PYTHON=python3&amp;#39; &amp;gt;&amp;gt; ~/.bash_profile
echo &amp;#39;export PYSPARK_DRIVER_PYTHON=python3&amp;#39; &amp;gt;&amp;gt; ~/.bash_profile
source ~/.bash_profile
cp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.properties  # minimize the Verbosity of Spark
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Linux:&lt;/h5&gt;
&lt;p&gt;Create a bash profile startup script located at &lt;code&gt;/etc/profile.d/spark.sh&lt;/code&gt; and add the environment variables&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;echo &amp;#39;export SPARK_HOME=~/spark&amp;#39; &amp;gt;&amp;gt; /etc/profile.d/spark.sh
echo &amp;#39;export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip&amp;#39; &amp;gt;&amp;gt; /etc/profile.d/spark.sh
echo &amp;#39;export PATH=$PATH:$SPARK_HOME/bin&amp;#39; &amp;gt;&amp;gt; /etc/profile.d/spark.sh
echo &amp;#39;export PYSPARK_PYTHON=python3&amp;#39; &amp;gt;&amp;gt; /etc/profile.d/spark.sh
echo &amp;#39;export PYSPARK_DRIVER_PYTHON=python3&amp;#39; &amp;gt;&amp;gt; /etc/profile.d/spark.sh
source /etc/profile.d/spark.sh
cp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.properties  # minimize the Verbosity of Spark
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Install Jupyter Notebook&lt;/h2&gt;
&lt;p&gt;Install with &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip3 install jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Install a Spark kernel for Jupyter Notebook&lt;/h2&gt;
&lt;p&gt;There are three ways to add Spark kernels in Jupyter notebooks&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PySpark with IPythonKernel&lt;/li&gt;
&lt;li&gt;&lt;a href="https://toree.apache.org"&gt;Apache Toree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jupyter-incubator/sparkmagic"&gt;Sparkmagic&lt;/a&gt; kernel&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;PySpark with IPythonKernel&lt;/h3&gt;
&lt;p&gt;Setting up PySpark in Jupyter is the easiest way to get started with interactive Spark sessions.&lt;/p&gt;
&lt;p&gt;Define a new kernel by creating a JSON file at: &lt;code&gt;/usr/local/share/jupyter/kernels/pyspark/kernel.json&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;display_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;PySpark&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;argv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;/usr/bin/python3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;ipykernel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;{connection_file}&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;quot;env&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SPARK_HOME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;~/spark/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONPATH&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;~/spark/python/:~/spark/python/lib/py4j-0.x.x.x-src.zip&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYTHONSTARTUP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;~/spark/python/pyspark/shell.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PYSPARK_SUBMIT_ARGS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--master local[*] --conf spark.executor.cores=1 --conf spark.executor.memory=512m pyspark-shell&amp;quot;&lt;/span&gt;
 &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By adding these environment variables, calling the &lt;code&gt;pyspark&lt;/code&gt; executable directly will launch Jupyter Notebook with PySpark kernel&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYSPARK_PYTHON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;python3
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYSPARK_DRIVER_PYTHON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jupyter&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYSPARK_DRIVER_PYTHON_OPTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;notebook&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Apache Toree&lt;/h3&gt;
&lt;p&gt;Toree is an Apache Incubating project originally created by developers at IBM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note!&lt;/strong&gt; A pip-installable package isn't currently available for Toree v0.2.0 which is required for Spark 2.x support.  Therefore we need to build and package up Toree.  This requires &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; to be installed.&lt;/p&gt;
&lt;p&gt;Setup the Docker repository&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum makecache fast
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Install the latest version of Docker Community Edition&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;yum -y install docker-ce
usermod -aG docker $(whoami)  # add your user to the &amp;#39;docker&amp;#39; group
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Start Docker&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;systemctl start docker
systemctl enable docker
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Clone and build Toree from the Toree repository from GitHub.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/apache/incubator-toree.git
cd incubator-toree/
APACHE_SPARK_VERSION=2.1.1 make pip-release
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Install Toree from the archive&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip3 install dist/toree-pip/toree-0.2.0.dev1.tar.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Add all the Toree kernels to Jupyter&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter toree install --spark_home&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SPARK_HOME&lt;/span&gt; --interpreters&lt;span class="o"&gt;=&lt;/span&gt;Scala,PySpark,SparkR,SQL --python_exec&lt;span class="o"&gt;=&lt;/span&gt;python3 --spark_opts&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--master=local[*]&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Confirm installation by listing the available kernels&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter kernelspec list
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Launch Jupyter Notebook&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter notebook
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Sparkmagic&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/jupyter-incubator/sparkmagic"&gt;Sparkmagic&lt;/a&gt; is a set of tools for interactively working with remote Spark clusters through &lt;a href="http://livy.io"&gt;Livy&lt;/a&gt;, a Spark REST server, in Jupyter notebooks. The Sparkmagic project includes a set of magics for interactively running Spark code in multiple languages, as well as some kernels that you can use to turn Jupyter into an integrated Spark environment.&lt;/p&gt;</content><category term="apache"></category><category term="spark"></category><category term="pyspark"></category><category term="python"></category><category term="jupyter"></category></entry><entry><title>Sqoop and Hive Best Practices</title><link href="http://justinnaldzin.github.io/sqoop-and-hive-best-practices.html" rel="alternate"></link><published>2017-04-07T00:00:00-04:00</published><updated>2017-04-07T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2017-04-07:/sqoop-and-hive-best-practices.html</id><summary type="html">&lt;p&gt;The following describes the HDFS architecture and best practices of using Sqoop and Hive to load data from relational databases.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following describes the HDFS architecture and best practices of using Sqoop and Hive to load data from relational databases.&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;h4&gt;Overview&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tables brought into HDFS using Sqoop should be imported into a staging area using a temporary external table.&lt;/li&gt;
&lt;li&gt;Hive SQL will be used to select from the external staging table and insert the data into the production table.&lt;/li&gt;
&lt;li&gt;Data in staging will be deleted upon successful load to production.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;HDFS&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HDFS Location: &lt;code&gt;/data/&amp;lt;schema&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Staging: &lt;code&gt;/data/&amp;lt;schema&amp;gt;/staging&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Production: &lt;code&gt;/data/&amp;lt;schema&amp;gt;/production&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Avro Schema: &lt;code&gt;/metadata/&amp;lt;schema&amp;gt;/&amp;lt;table&amp;gt;.avsc&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Hive&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;External tables should be created to point to HDFS locations within the production HDFS directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Partitioning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Where applicable, data should be partitioned by a &lt;code&gt;date&lt;/code&gt; column.&lt;/li&gt;
&lt;li&gt;Use Hive's dynamic partitioning feature to automatically create partitions on data insert.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Workflow&lt;/h2&gt;
&lt;h4&gt;Setup&lt;/h4&gt;
&lt;p&gt;Define project variables&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;HOSTNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;hostname&amp;gt;
&lt;span class="nv"&gt;USERNAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;username&amp;gt;
&lt;span class="nv"&gt;DATABASE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;database&amp;gt;
&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;
&lt;span class="nv"&gt;TABLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;table&amp;gt;
&lt;span class="nv"&gt;STAGE_TABLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;STG_&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create HDFS file structure&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hadoop fs -mkdir /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/staging/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
hadoop fs -mkdir /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/production/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
hadoop fs -chown hive:hdfs /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/staging/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
hadoop fs -chown hive:hdfs /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/production/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
hadoop fs -chmod -R &lt;span class="m"&gt;777&lt;/span&gt; /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/staging/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
hadoop fs -chmod -R &lt;span class="m"&gt;777&lt;/span&gt; /data/&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;/production/&lt;span class="nv"&gt;$TABLE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a Hive Database named the same as the HDFS Schema above&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;CREATE DATABASE IF NOT EXISTS $DATABASE;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Staging&lt;/h4&gt;
&lt;p&gt;Sqoop Import into HDFS&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop import --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$HOSTNAME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;;database=&amp;#39;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &lt;span class="nv"&gt;$USERNAME&lt;/span&gt; -P --verbose &lt;span class="se"&gt;\&lt;/span&gt;
--table &lt;span class="nv"&gt;$TABLE&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--as-avrodatafile &lt;span class="se"&gt;\&lt;/span&gt;
--compress &lt;span class="se"&gt;\&lt;/span&gt;
--verbose &lt;span class="se"&gt;\&lt;/span&gt;
--target-dir &lt;span class="s2"&gt;&amp;quot;/data/&lt;/span&gt;&lt;span class="nv"&gt;$SCHEMA&lt;/span&gt;&lt;span class="s2"&gt;/staging/&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--delete-target-dir  &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt; &lt;span class="nv"&gt;$TABLE&lt;/span&gt;.log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copy the AVRO schema to HDFS&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hadoop fs -put &lt;span class="nv"&gt;$TABLE&lt;/span&gt;.avsc /metadata/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.avsc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run the Hive DDL&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;CREATE EXTERNAL TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$STAGE_TABLE&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;ROW FORMAT SERDE&lt;/span&gt;
&lt;span class="s2"&gt;&amp;#39;org.apache.hadoop.hive.serde2.avro.AvroSerDe&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;STORED AS INPUTFORMAT&lt;/span&gt;
&lt;span class="s2"&gt;&amp;#39;org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;OUTPUTFORMAT&lt;/span&gt;
&lt;span class="s2"&gt;&amp;#39;org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;LOCATION &amp;#39;/data/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/stage/&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;TBLPROPERTIES (&lt;/span&gt;
&lt;span class="s2"&gt;&amp;#39;avro.schema.url&amp;#39;=&amp;#39;/metadata/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.avsc&amp;#39;);&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Test the Staging table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;SELECT * FROM &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$STAGE_TABLE&lt;/span&gt;&lt;span class="s2"&gt; LIMIT 10;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Production&lt;/h4&gt;
&lt;p&gt;Run the Hive DDL for &lt;strong&gt;non-partitioned&lt;/strong&gt; table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;CREATE EXTERNAL TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt;(&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;lt;COLUMN_1&amp;gt; &amp;lt;DATATYPE&amp;gt;,&lt;/span&gt;
&lt;span class="s2"&gt;    ...&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;lt;COLUMN_n&amp;gt; &amp;lt;DATATYPE&amp;gt;&lt;/span&gt;
&lt;span class="s2"&gt;)&lt;/span&gt;
&lt;span class="s2"&gt;STORED AS ORC&lt;/span&gt;
&lt;span class="s2"&gt;LOCATION &amp;#39;/data/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/production/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run the Hive DDL for &lt;strong&gt;partitioned&lt;/strong&gt; table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;CREATE EXTERNAL TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt;(&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;lt;COLUMN_1&amp;gt; &amp;lt;DATATYPE&amp;gt;,&lt;/span&gt;
&lt;span class="s2"&gt;    ...&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;lt;COLUMN_n&amp;gt; &amp;lt;DATATYPE&amp;gt;&lt;/span&gt;
&lt;span class="s2"&gt;)&lt;/span&gt;
&lt;span class="s2"&gt;PARTITIONED BY (dt date)&lt;/span&gt;
&lt;span class="s2"&gt;STORED AS ORC&lt;/span&gt;
&lt;span class="s2"&gt;LOCATION &amp;#39;/data/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SCHEMA&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/production/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copy data from Staging to Production for &lt;strong&gt;non-partitioned&lt;/strong&gt; table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;USE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;;&amp;quot;&lt;/span&gt;
hive -e &lt;span class="s2"&gt;&amp;quot;INSERT INTO TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;         SELECT * FROM &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$STAGE_TABLE&lt;/span&gt;&lt;span class="s2"&gt;;&amp;quot;&lt;/span&gt;
hive -e &lt;span class="s2"&gt;&amp;quot;ANALYZE TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt; COMPUTE STATISTICS FOR COLUMNS;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copy data from Staging to Production for &lt;strong&gt;partitioned&lt;/strong&gt; table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;SET hive.exec.dynamic.partition=true;&amp;quot;&lt;/span&gt;
hive -e &lt;span class="s2"&gt;&amp;quot;USE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;;&amp;quot;&lt;/span&gt;
hive -e &lt;span class="s2"&gt;&amp;quot;INSERT INTO TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt; PARTITION(dt)&lt;/span&gt;
&lt;span class="s2"&gt;         SELECT *, CAST(&amp;lt;COLUMN&amp;gt; AS DATE) AS dt FROM &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$STAGE_TABLE&lt;/span&gt;&lt;span class="s2"&gt;;&amp;quot;&lt;/span&gt;
hive -e &lt;span class="s2"&gt;&amp;quot;ANALYZE TABLE &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$TABLE&lt;/span&gt;&lt;span class="s2"&gt; PARTITION(dt) COMPUTE STATISTICS FOR COLUMNS;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Truncate Staging table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hive -e &lt;span class="s2"&gt;&amp;quot;TRUNCATE TABLE IF EXISTS &lt;/span&gt;&lt;span class="nv"&gt;$DATABASE&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="nv"&gt;$STAGE_TABLE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Incremental load process should run with &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; or cron.&lt;/p&gt;</content><category term="sqoop"></category><category term="hive"></category><category term="hadoop"></category><category term="hdfs"></category></entry><entry><title>Sqoop Guide</title><link href="http://justinnaldzin.github.io/sqoop-guide.html" rel="alternate"></link><published>2017-04-01T00:00:00-04:00</published><updated>2017-04-01T00:00:00-04:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2017-04-01:/sqoop-guide.html</id><summary type="html">&lt;p&gt;The following guide explains how to use Sqoop to transfer data from relational databases to Hadoop HDFS&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The following guide explains how to use Sqoop to transfer data from relational databases to Hadoop HDFS&lt;/p&gt;
&lt;h2&gt;What is Sqoop?&lt;/h2&gt;
&lt;p&gt;Sqoop is a command-line tool designed to transfer data between relational database servers and Hadoop.  It has the ability to import and export data between Hadoop HDFS and multiple relational databases such as MySQL, Oracle, and SQL Server.&lt;/p&gt;
&lt;h2&gt;Installing JDBC Drivers&lt;/h2&gt;
&lt;p&gt;Download and install the &lt;strong&gt;MySQL&lt;/strong&gt; JDBC Driver&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -L &lt;span class="s1"&gt;&amp;#39;https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.41.tar.gz&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; tar xz
sudo cp mysql-connector-java-5.1.41/mysql-connector-java-5.1.41-bin.jar /usr/hdp/current/sqoop-client/lib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Download and install the &lt;strong&gt;Oracle&lt;/strong&gt; JDBC Driver&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -L &lt;span class="s1"&gt;&amp;#39;http://download.oracle.com/otn/utilities_drivers/jdbc/11204/ojdbc6.jar&amp;#39;&lt;/span&gt;
sudo cp ojdbc6.jar /usr/hdp/current/sqoop-client/lib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Download and install the &lt;strong&gt;Microsoft SQL Server&lt;/strong&gt; JDBC Driver&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -L &lt;span class="s1"&gt;&amp;#39;https://download.microsoft.com/download/0/2/A/02AAE597-3865-456C-AE7F-613F99F850A8/enu/sqljdbc_6.0.81cd12.100_enu.tar.gz&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; tar xz
sudo cp sqljdbc_6.0/enu/jre8/sqljdbc42.jar /usr/hdp/current/sqoop-client/lib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Alternatively specify the classpath to the driver&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HADOOP_CLASSPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/jars/sqljdbc4.jar
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sqoop basics&lt;/h2&gt;
&lt;p&gt;Check sqoop version&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop version
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;List of commands&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop &lt;span class="nb"&gt;help&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;More command specific&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop &lt;span class="nb"&gt;help&lt;/span&gt; import
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;List databases&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop list-databases --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;&amp;#39;&lt;/span&gt; --username &amp;lt;username&amp;gt; -P --verbose
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;List tables&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop list-tables --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; -P --verbose
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Transfer relational database tables into Hadoop HDFS&lt;/h2&gt;
&lt;h4&gt;Sqoop Import into HDFS&lt;/h4&gt;
&lt;p&gt;Import from a table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop import --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;\;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; -P --verbose
--table &amp;lt;table&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--as-avrodatafile &lt;span class="se"&gt;\&lt;/span&gt;
--compress &lt;span class="se"&gt;\&lt;/span&gt;
--verbose &lt;span class="se"&gt;\&lt;/span&gt;
--target-dir &lt;span class="s2"&gt;&amp;quot;/data/&amp;lt;schema&amp;gt;/staging/&amp;lt;table&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Import from a table with &lt;code&gt;where&lt;/code&gt; conditions&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop import --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;\;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; -P --verbose
--table &amp;lt;table&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--as-avrodatafile &lt;span class="se"&gt;\&lt;/span&gt;
--compress &lt;span class="se"&gt;\&lt;/span&gt;
--verbose &lt;span class="se"&gt;\&lt;/span&gt;
--target-dir &lt;span class="s2"&gt;&amp;quot;/data/&amp;lt;schema&amp;gt;/staging/&amp;lt;table&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--where &lt;span class="s2"&gt;&amp;quot;1=1&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--num-mappers &amp;lt;n&amp;gt;  &lt;span class="c1"&gt;#  Use &amp;#39;n&amp;#39; map tasks to import in parallel&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Import from a table specifying a query&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop import --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;\;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; -P --verbose
--table &amp;lt;table&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--as-avrodatafile &lt;span class="se"&gt;\&lt;/span&gt;
--compress &lt;span class="se"&gt;\&lt;/span&gt;
--verbose &lt;span class="se"&gt;\&lt;/span&gt;
--target-dir &lt;span class="s2"&gt;&amp;quot;/data/&amp;lt;schema&amp;gt;/staging/&amp;lt;table&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--query &lt;span class="s1"&gt;&amp;#39;SELECT * FROM &amp;lt;table&amp;gt; WHERE 1=1&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--delete-target-dir  &lt;span class="c1"&gt;# Overwrite target directory if it already exists&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Sqoop Job&lt;/h4&gt;
&lt;p&gt;Create a Sqoop Job&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop job --create &amp;lt;jobname&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
-- &lt;span class="se"&gt;\&lt;/span&gt;
import &lt;span class="se"&gt;\&lt;/span&gt;
--connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;\;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; --password-file .secrets &lt;span class="se"&gt;\&lt;/span&gt;
--table &amp;lt;table&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--check-column &amp;lt;primarykey&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--as-avrodatafile &lt;span class="se"&gt;\&lt;/span&gt;
--compress &lt;span class="se"&gt;\&lt;/span&gt;
--verbose &lt;span class="se"&gt;\&lt;/span&gt;
--target-dir &lt;span class="s2"&gt;&amp;quot;/data/&amp;lt;schema&amp;gt;/staging/&amp;lt;table&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--incremental append &lt;span class="se"&gt;\&lt;/span&gt;
--last-value &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--split-by &amp;lt;primarykey&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a password file&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; -n &amp;lt;password&amp;gt; &amp;gt; .secrets
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Verify Job&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop job --list
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Inspect Job&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop job --show &amp;lt;jobname&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Execute Job&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop job --exec &amp;lt;jobname&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Sqoop Import into Hive&lt;/h4&gt;
&lt;p&gt;Import from a table and create Hive table&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sqoop import --connect &lt;span class="s1"&gt;&amp;#39;jdbc:sqlserver://&amp;lt;hostname&amp;gt;\;database=&amp;lt;database&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--username &amp;lt;username&amp;gt; -P --verbose &lt;span class="se"&gt;\&lt;/span&gt;
--table &amp;lt;table&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
--hive-import &lt;span class="se"&gt;\&lt;/span&gt;
--create-hive-table
&lt;/pre&gt;&lt;/div&gt;</content><category term="sqoop"></category><category term="hadoop"></category><category term="hdfs"></category></entry><entry><title>Create a website using GitHub Pages and Pelican</title><link href="http://justinnaldzin.github.io/create-a-website-using-github-pages-and-pelican.html" rel="alternate"></link><published>2017-02-01T00:00:00-05:00</published><updated>2017-02-01T00:00:00-05:00</updated><author><name>Justin Naldzin</name></author><id>tag:justinnaldzin.github.io,2017-02-01:/create-a-website-using-github-pages-and-pelican.html</id><summary type="html">&lt;p&gt;Using Pelican with GitHub Pages, this guide shows how to set up a static site for hosting and sharing projects.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Using &lt;a href="http://docs.getpelican.com"&gt;Pelican&lt;/a&gt; with &lt;a href="https://pages.github.com"&gt;GitHub Pages&lt;/a&gt;, this guide shows how to set up a static site for hosting and sharing projects.&lt;/p&gt;
&lt;p&gt;Pelican is a static site generator, written in Python, that requires no database or server-side logic.&lt;/p&gt;
&lt;p&gt;GitHub Pages allows you to host website content directly from your GitHub repository.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Create a Pelican source project repository&lt;/h3&gt;
&lt;p&gt;Login to your GitHub account and create a new repository named &lt;code&gt;pelican-github-pages&lt;/code&gt;.  This will be the Pelican source project that will generate the static HTML pages.&lt;/p&gt;
&lt;p&gt;Clone the repo&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/username/pelican-github-pages
&lt;span class="nb"&gt;cd&lt;/span&gt; pelican-github-pages
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h2&gt;Create a GitHub Pages repo&lt;/h2&gt;
&lt;p&gt;Head over to &lt;a href="https://pages.github.com/"&gt;GitHub Pages &lt;/a&gt; and follow the simple instructions to create a new repository named &lt;code&gt;username.github.io&lt;/code&gt; substituting your GitHub username, obviously.  This will contain the HTML files for the static site that GitHub hosts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Install and setup Pelican&lt;/h2&gt;
&lt;p&gt;Install Pelican and Markdown&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pelican markdown
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a skeleton project via the &lt;code&gt;pelican-quickstart&lt;/code&gt; script&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is an example of the script output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Welcome to pelican-quickstart v3.7.1.

This script will help you create a new Pelican-based website.

Please answer the following questions so this script can generate the files
needed by Pelican.


&amp;gt; Where do you want to create your new web site? [.]
&amp;gt; What will be the title of this web site? Justin Naldzin
&amp;gt; Who will be the author of this web site? Justin Naldzin
&amp;gt; What will be the default language of this web site? [en]
&amp;gt; Do you want to specify a URL prefix? e.g., http://example.com   (Y/n) y
&amp;gt; What is your URL prefix? (see above example; no trailing slash) http://justinnaldzin.github.io
&amp;gt; Do you want to enable article pagination? (Y/n) y
&amp;gt; How many articles per page do you want? [10]
&amp;gt; What is your time zone? [Europe/Paris] America/New_York
&amp;gt; Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) y
&amp;gt; Do you want an auto-reload &amp;amp; simpleHTTP script to assist with theme and site development? (Y/n) y
&amp;gt; Do you want to upload your website using FTP? (y/N) n
&amp;gt; Do you want to upload your website using SSH? (y/N) n
&amp;gt; Do you want to upload your website using Dropbox? (y/N) n
&amp;gt; Do you want to upload your website using S3? (y/N) n
&amp;gt; Do you want to upload your website using Rackspace Cloud Files? (y/N) n
&amp;gt; Do you want to upload your website using GitHub Pages? (y/N) y
&amp;gt; Is this your personal page (username.github.io)? (y/N) y
Done. Your new project is available at /Users/justin/Desktop/pelican-github-pages
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a Markdown document saving with a &lt;code&gt;.md&lt;/code&gt; extension within the &lt;code&gt;content&lt;/code&gt; directory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Hello&lt;/span&gt; &lt;span class="n"&gt;World&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Guides&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;about&lt;/span&gt;

&lt;span class="n"&gt;Hello&lt;/span&gt; &lt;span class="n"&gt;World&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h2&gt;Pelican Themes&lt;/h2&gt;
&lt;p&gt;Add the &lt;strong&gt;Pelican Themes&lt;/strong&gt; repo as a submodule of your Pelican source project&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git submodule add https://github.com/getpelican/pelican-themes
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also add the &lt;strong&gt;Pelican Plugins&lt;/strong&gt; repo as a submodule&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git submodule add https://github.com/getpelican/pelican-plugins
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Set the &lt;code&gt;THEME&lt;/code&gt; variable in your &lt;code&gt;pelicanconf.py&lt;/code&gt; file to the absolute or relative path to the theme.  For example, here is the &lt;strong&gt;Bootstrap 3&lt;/strong&gt; theme:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;THEME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;pelican-themes/pelican-bootstrap3&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And set the following variables to support the Bootstrap 3 theme&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;JINJA_EXTENSIONS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;jinja2.ext.i18n&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;PLUGIN_PATHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pelican-plugins&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;i18n_subsites&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Add the GitHub pages repo that you created above as a submodule, naming it &lt;code&gt;output&lt;/code&gt; to match Pelican's default output directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git submodule add https://github.com/username/username.github.io output
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Change the following setting to False in your &lt;code&gt;publishconf.py&lt;/code&gt; file to prevent the deletion of the output directory when running the &lt;code&gt;pelican&lt;/code&gt; command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DELETE_OUTPUT_DIRECTORY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; False
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h2&gt;Add support for Jupyter Notebooks&lt;/h2&gt;
&lt;p&gt;To have Pelican automatically convert Jupyter Notebooks (files with the extention .ipynb) into posts, we need to install a &lt;a href="https://github.com/danielfrg/pelican-ipynb"&gt;plugin&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;Install the Jupyter plugin as a git submodule&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git submodule add git://github.com/danielfrg/pelican-ipynb.git pelican-plugins/jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Modify &lt;code&gt;pelicanconf.py&lt;/code&gt; to activate the plugin&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PLUGIN_PATH = &amp;#39;./pelican-plugins&amp;#39;
PLUGINS = [&amp;#39;ipynb.markup&amp;#39;]

# Jupyter
MARKUP = (&amp;#39;md&amp;#39;, &amp;#39;ipynb&amp;#39;)
IPYNB_USE_META_SUMMARY = True
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h2&gt;Convert Jupyter Notebooks into posts&lt;/h2&gt;
&lt;p&gt;Copy the notebook into the &lt;code&gt;content&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;Within the &lt;code&gt;content&lt;/code&gt; folder, create a new metadata file with the same name as the &lt;code&gt;.ipynb&lt;/code&gt; file but with the extension &lt;code&gt;.ipynb-meta&lt;/code&gt;.  This file should only include the post's metadata.  For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;Jupyter&lt;/span&gt; &lt;span class="n"&gt;Notebook&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Guides&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;h2&gt;Generating the static site files and run a devserver&lt;/h2&gt;
&lt;p&gt;Create additional Markdown files saving with a &lt;code&gt;.md&lt;/code&gt; extension within the &lt;code&gt;content&lt;/code&gt; directory.  For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Hello&lt;/span&gt; &lt;span class="n"&gt;World&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2017&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Guide&lt;/span&gt;

&lt;span class="n"&gt;Hello&lt;/span&gt; &lt;span class="n"&gt;World&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Within the project's main directory, generate the HTML files and serve the site locally&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make devserver
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Navigate to http://localhost:8000&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Publish the static HTML files to GitHub&lt;/h2&gt;
&lt;p&gt;Initialize the output directory, add the remote, add the files, commit, and push the changes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; output
$ git init
$ git remote add origin https://github.com/username/username.github.io.git
$ git add --all
$ git commit -m &lt;span class="s2"&gt;&amp;quot;inital commit&amp;quot;&lt;/span&gt;
$ git push origin master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now navigate to http://username.github.io&lt;/p&gt;
&lt;p&gt;Also push the Pelican source project to the &lt;code&gt;pelican-github-pages&lt;/code&gt; repo&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ..
git add .
git commit -m &lt;span class="s2"&gt;&amp;quot;First commit.&amp;quot;&lt;/span&gt;
git push -u origin master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Add a pre-push git hook to the Pelican source repo that handles the automatic pushing of the GitHub Pages repo.&lt;br&gt;
This file needs to be placed in the Pelican source directory at &lt;code&gt;.git/hooks/pre-push&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nv"&gt;lastcommit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;git log origin/master -1 --oneline&lt;span class="sb"&gt;`&lt;/span&gt;
make publish
&lt;span class="nb"&gt;cd&lt;/span&gt; output &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git add -A &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git commit -m &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$lastcommit&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git push origin master
&lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="pelican"></category><category term="python"></category><category term="github"></category></entry></feed>